{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder based time-series explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "# from matplotlib import gridspec\n",
    "# from skimage.transform import resize\n",
    "# from tqdm import tqdm\n",
    "from scipy import linalg\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.interpolate import interp1d\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import platform\n",
    "import sys\n",
    "from math import sqrt\n",
    "import matplotlib\n",
    "SPINE_COLOR = 'gray'\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "__file__ = '/home/s7thakur/ecresearch-shared/PhysioNet/resnet1d/resnet1d'\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))\n",
    "\n",
    "from resnet1d.resnet1d import MyDataset, ResNet1D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.utils import reshape, label_encoding, calculate_metrics, smooth, norm, read_dataset_mitdb, torch_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/home/s7thakur/ecresearch-shared/compute1/sktime-dl')\n",
    "# from sktime_dl.deeplearning.inceptiontime._classifier import InceptionTimeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/s7thakur/ecresearch-shared/compute1/RISE/')\n",
    "from utils_ts import read_dataset, reshape, label_encoding, calculate_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils_ts import latexify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform \n",
    "print(platform.node())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change code below to incorporate your *model* and *input processing*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your model here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# datasets = ['PhalangesOutlinesCorrect', 'CinCECGTorso','ItalyPowerDemand', 'Trace', 'GunPoint', 'GunPointAgeSpan', \n",
    "#                             'Strawberry', 'ECGFiveDays', 'TwoLeadECG', 'Chinatown', 'DistalPhalanxOutlineCorrect']\n",
    "\n",
    "# dataset = datasets[3]\n",
    "\n",
    "# if platform.node() == \"compute3.esg.uwaterloo.ca\":\n",
    "#     if os.listdir('/home/s7thakur/ecresearch-shared/compute1'):\n",
    "#         filepath = '/home/s7thakur/ecresearch-shared/compute1'\n",
    "    \n",
    "#     else: \n",
    "#         print('ecresearch-shared is not mounted')\n",
    "        \n",
    "# elif platform.node() == \"compute1.esg.uwaterloo.ca\":\n",
    "#     if not os.listdir('rhome/s7thakur/ecresearch-s7thakur/compute1'):\n",
    "#         filepath = '/rhome/s7thakur/ecresearch-s7thakur/compute1'\n",
    "#         print(filepath)\n",
    "#     else: \n",
    "#         print('ecresearch-shared is not mounted')\n",
    "    \n",
    "# else: print('file path does not exist')\n",
    "\n",
    "dataset_name='mitdb'\n",
    "archive_name = 'PhysioNet'\n",
    "filepath='/home/s7thakur/ecresearch-shared'\n",
    "    \n",
    "archive_dir=os.path.join(filepath,'data','PhysioNet')\n",
    "model_dir = os.path.join(filepath, 'PhysioNet')\n",
    "\n",
    "\n",
    "# dir = os.path.join('/rhome/s7thakur/MIRROR/logs/autoencoder/saliency/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    \"\"\"\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into       \n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    # initialize optimizer from checkpoint to optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
    "    valid_loss_min = checkpoint['valid_loss_min']\n",
    "    # return model, optimizer, epoch value, min validation loss \n",
    "    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device_str = \"cuda:0\"\n",
    "device = torch.device(device_str if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = ResNet1D(\n",
    "    in_channels=1, \n",
    "    base_filters=128, \n",
    "    kernel_size=16, \n",
    "    stride=2, \n",
    "    groups=8, \n",
    "    n_block=8, \n",
    "    n_classes=len(np.unique(Y_train)), \n",
    "    downsample_gap=2, \n",
    "    increasefilter_gap=4, \n",
    "    use_do=False)\n",
    "\n",
    "# summary(model, torch.zeros(1, 1, 360))\n",
    "model.to(device)\n",
    "\n",
    "# define optimzer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# define checkpoint saved path\n",
    "ckp_path = os.path.join(model_dir,\"current_checkpoint_two_class.pt\")\n",
    "\n",
    "# load the saved checkpoint\n",
    "model, optimizer, start_epoch, valid_loss_min = load_ckp(ckp_path, model, optimizer)\n",
    "\n",
    "print(\"model = \", model)\n",
    "print(\"optimizer = \", optimizer)\n",
    "print(\"start_epoch = \", start_epoch)\n",
    "print(\"valid_loss_min = \", valid_loss_min)\n",
    "print(\"valid_loss_min = {:.6f}\".format(valid_loss_min))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "### Input: $x \\in \\mathcal{R}^d$, Class: $y \\in \\mathcal{R}^c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2index(i):\n",
    "#     m = {'N':0, 'S':1, 'V':2, 'F':3, 'Q':4} # uncomment for 5 classes\n",
    "    m = {'N':0, 'S':0, 'V':1, 'F':0, 'Q':0} # uncomment for 2 classes\n",
    "    return m[i]\n",
    "batch_size=256\n",
    "data = np.load(os.path.join(filepath, archive_name, 'data', 'mitdb_data.npy'))\n",
    "label_str = np.load(os.path.join(filepath, archive_name, 'data', 'mitdb_group.npy'))\n",
    "\n",
    "print(np.unique(label_str))\n",
    "label = np.array([label2index(i) for i in label_str])\n",
    "\n",
    "# make data\n",
    "train_ind = np.load(os.path.join(filepath, archive_name, 'data', 'mitdb_train_ind.npy'))\n",
    "test_ind = np.load(os.path.join(filepath, archive_name, 'data', 'mitdb_test_ind.npy'))\n",
    "# data = preprocessing.scale(data, axis=1)\n",
    "\n",
    "X_train = data[train_ind]\n",
    "X_test = data[test_ind]\n",
    "Y_train = label[train_ind]\n",
    "Y_test = label[test_ind]\n",
    "\n",
    "std_ = X_train.std(axis=1, keepdims=True)\n",
    "std_[std_ == 0] = 1.0\n",
    "X_train = (X_train - X_train.mean(axis=1, keepdims=True)) / std_\n",
    "\n",
    "std_ = X_test.std(axis=1, keepdims=True)\n",
    "std_[std_ == 0] = 1.0\n",
    "X_test = (X_test - X_test.mean(axis=1, keepdims=True)) / std_\n",
    "\n",
    "print(X_train.shape, Counter(Y_train))\n",
    "print(X_test.shape, Counter(Y_test))\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train, Y_train = ros.fit_resample(X_train, Y_train)\n",
    "print(X_train.shape, Counter(Y_train))\n",
    "print(np.max(X_train), np.min(X_train))\n",
    "# for i in range(20):\n",
    "#     plt.figure()\n",
    "#     idx = np.random.randint(X_train.shape[0])\n",
    "#     title = '{}_{}'.format(Y_train[idx], idx)\n",
    "#     plt.plot(X_train[idx])\n",
    "#     plt.title(title)\n",
    "#     plt.savefig('img/{0}.png'.format(title))\n",
    "# exit()\n",
    "\n",
    "# prepare loader\n",
    "shuffle_idx = np.random.permutation(list(range(X_train.shape[0])))\n",
    "X_train = X_train[shuffle_idx]\n",
    "Y_train = Y_train[shuffle_idx]\n",
    "X_train = np.expand_dims(X_train, 1)\n",
    "X_test = np.expand_dims(X_test, 1)\n",
    "dataset = MyDataset(X_train, Y_train)\n",
    "# dataset_test = MyDataset(X_test, Y_test)\n",
    "# print(X_train[1550:1600].shape, Y_train[1550:1600].shape, Counter(Y_train[1550:1600]))\n",
    "dataset_test = MyDataset(X_train[1550:1600], Y_train[1550:1600])\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, drop_last=False, shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=1, drop_last=False, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataloader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_acc = 0.0\n",
    "for samples, labels in dataloader_test:\n",
    "    with torch.no_grad():\n",
    "        samples, labels = samples.cuda(), labels.cuda()\n",
    "        \n",
    "#         print(samples.shape, labels.shape)\n",
    "        output = model(samples)\n",
    "        # calculate accuracy\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        correct = pred.eq(labels)\n",
    "        test_acc += torch.mean(correct.float())\n",
    "        # print('output {}'.format(output))\n",
    "        # print('correct {}'.format(correct))\n",
    "        # print('predict {}'.format(pred))\n",
    "\n",
    "        # print('test Accuracy {}'.format(test_acc))\n",
    "print(len(dataloader_test))\n",
    "print('Accuracy of the network on {} test images: {}%'.format(len(labels), round(test_acc.item()*100.0/len(dataloader_test), 2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_test.shape[1:]\n",
    "    \n",
    "    \n",
    "N=len(dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x,window_len=11,window='hanning'):\n",
    "    \"\"\"smooth the data using a window with requested size.\n",
    "    \n",
    "    This method is based on the convolution of a scaled window with the signal.\n",
    "    The signal is prepared by introducing reflected copies of the signal \n",
    "    (with the window size) in both ends so that transient parts are minimized\n",
    "    in the begining and end part of the output signal.\n",
    "    \n",
    "    input:\n",
    "        x: the input signal \n",
    "        window_len: the dimension of the smoothing window; should be an odd integer\n",
    "        window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\n",
    "            flat window will produce a moving average smoothing.\n",
    "\n",
    "    output:\n",
    "        the smoothed signal\n",
    "        \n",
    "    example:\n",
    "\n",
    "    t=linspace(-2,2,0.1)\n",
    "    x=sin(t)+randn(len(t))*0.1\n",
    "    y=smooth(x)\n",
    "    \n",
    "    see also: \n",
    "    \n",
    "    numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve\n",
    "    scipy.signal.lfilter\n",
    " \n",
    "    TODO: the window parameter could be the window itself if an array instead of a string\n",
    "    NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.\n",
    "    \"\"\"\n",
    "\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError\n",
    "\n",
    "    if x.size < window_len:\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "    if window_len<3:\n",
    "        return x\n",
    "\n",
    "\n",
    "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
    "\n",
    "    if window == 'flat': #moving average\n",
    "        w=np.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval('np.'+window+'(window_len)')\n",
    "\n",
    "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_0=[]\n",
    "x_1=[]\n",
    "labels = np.unique(Y_train)\n",
    "for samples, labels in dataloader_test:\n",
    "    with torch.no_grad():\n",
    "        samples, labels = samples.cuda(), labels.cuda()\n",
    "        print(labels.cpu().numpy())\n",
    "        print(samples.cpu().numpy()[0][0].shape)\n",
    "        if labels.cpu().numpy()[-1] == 0:\n",
    "            \n",
    "            if len(x_0) ==5:\n",
    "                continue\n",
    "            else:\n",
    "                x_0.append(samples.cpu().numpy()[0][0])\n",
    "        elif labels.cpu().numpy()[-1] == 1:\n",
    "            if len(x_1) ==5:\n",
    "                continue\n",
    "            else:\n",
    "                x_1.append(samples.cpu().numpy()[0][0])\n",
    "        if len(x_1)==5:\n",
    "            break\n",
    "print(np.array(x_1).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array(x_0)\n",
    "x_1 = np.array(x_1)\n",
    "\n",
    "plt.figure()\n",
    "for x in x_0:\n",
    "#     print(x.shape)\n",
    "    plt.plot(x_0.T, c='g')\n",
    "for x in x_1:\n",
    "    plt.plot(x_1.T,c='r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.unique(Y_train)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random samples ($s \\in \\mathcal{S}^d$) where $\\mathcal{S} = \\mathcal{N}(0,1)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 500\n",
    "# w_size=20 works in general\n",
    "def gen_initial_mask(N, s,p=0.5):\n",
    "    \n",
    "#     print('(N,s)', N, s)\n",
    "#     grid = np.random.randn(s, 1) \n",
    "    grids = np.empty((N, s))\n",
    "    samples = np.random.normal(0,1, size=(N, s)) < p\n",
    "#     samples = min + (samples * (max - min))\n",
    "    samples = samples.astype('float32')\n",
    "    for i in range(0, N):\n",
    "        grids[i] = smooth(samples[i], 20)[0:s]\n",
    "    return grids\n",
    "\n",
    "\n",
    "def gen_initial_mask_binary(N, s,p=0.5,max_lobes=3, winsize=20,window='hamming'):\n",
    "    \n",
    "#     print('(N,s)', N, s)\n",
    "    high = int(s/(s/max_lobes))\n",
    "    win = int((winsize/100)*s)\n",
    "    low = np.random.randint(1,high=high, size=N)\n",
    "#     print(high, win)\n",
    "    rng = np.random.default_rng()\n",
    "    \n",
    "    samples = np.concatenate((np.ones((N,low[0])),np.zeros((N,s-low[0]))),axis=1)\n",
    "    for i,sample in enumerate(samples):\n",
    "    \n",
    "        rng.shuffle(sample)\n",
    "        \n",
    "        samples[i]=smooth(sample,win,window=window)[0:s]\n",
    "        \n",
    "    samples = samples.astype('float32')\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def gen_mask_binary(N, s,p=0.5,max_lobes=3, winsize=20,window='hamming'):\n",
    "    \n",
    "#     print('(N,s)', N, s)\n",
    "    high = int(s/(s/max_lobes))\n",
    "    win = int((winsize/100)*s)\n",
    "    lobes = np.random.randint(1,high=high, size=N)\n",
    "#     print(lobes)\n",
    "    rng = np.random.default_rng()\n",
    "    \n",
    "    samples = np.concatenate((np.ones((N,lobes[0])),np.zeros((N,s-lobes[0]))),axis=1)\n",
    "    for i,sample in enumerate(samples):\n",
    "    \n",
    "        rng.shuffle(sample)\n",
    "        \n",
    "        samples[i]=smooth(sample,win,window=window)[0:s]\n",
    "    \n",
    "    samples = samples.astype('float32')\n",
    "    \n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# samples = gen_initial_mask_binary(2,82,p=0.6,winsize=10,window='flat')\n",
    "samples = gen_mask_binary(2,82,p=0.6,max_lobes=5,winsize=20,window='flat')\n",
    "\n",
    "plt.figure()\n",
    "for sample in samples:\n",
    "    plt.plot(sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s=100\n",
    "print((s/10))\n",
    "low = np.random.randint(2,int(s/30), 1)\n",
    "print(5,low)\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "samples = np.concatenate((np.ones(low[0]),np.zeros((s-low[0]))),axis=None)\n",
    "print(samples)\n",
    "rng.shuffle(samples)\n",
    "print(samples)\n",
    "plt.plot(smooth(samples,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=100\n",
    "N=20\n",
    "rng = np.random.default_rng()\n",
    "print((s/20))\n",
    "low = np.random.randint(2,high=int(s/30), size=N)\n",
    "print(5,low)\n",
    "samples = np.concatenate((np.ones((N,low[0])),np.zeros((N,s-low[0]))),axis=1)\n",
    "print(samples.shape)\n",
    "for i,sample in enumerate(samples):\n",
    "#     print(sample.shape)\n",
    "    rng.shuffle(sample)\n",
    "    samples[i]=smooth(sample,40,window='hamming')[0:s]\n",
    "#     print(sample)\n",
    "plt.figure()\n",
    "for sample in samples:\n",
    "    plt.plot(sample)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm to weigh the input samples based on relevance by randomly subsampling input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import sklearn\n",
    "\n",
    "def norm(x):\n",
    "    \n",
    "    return  (x - np.mean(x))/np.std(x)\n",
    "\n",
    "def sigmoid(x):  \n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "    \n",
    "# bins = 10\n",
    "N = 1000\n",
    "\n",
    "\n",
    "    \n",
    "def calc_mse(x, y, z, recon_pred, masked_preds, test, class_idx, lamb):\n",
    "    x = x.reshape(x.shape[0], x.shape[1])\n",
    "    y = y.reshape(y.shape[0], y.shape[1])\n",
    "    grads_recon = np.gradient(y, axis=1)\n",
    "    grads_masked = np.gradient(np.square(np.abs(z-test)), axis=1)\n",
    "    mse1 =recon_preds.T.dot(( np.gradient(x, axis=1))*lamb)\n",
    "    mse2 = masked_preds.T.dot((grads_masked)*lamb)\n",
    "    mse=mse2\n",
    "    \n",
    "#     plt.figure()\n",
    "#     for ms in mse:\n",
    "#         print(ms.shape)\n",
    "#         plt.plot(ms)\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "    return mse\n",
    "\n",
    "\n",
    "\n",
    "if x_train.shape[0] < x_test.shape[0]:\n",
    "    test = x_train[0:50]\n",
    "    labels = y_train[0:50]\n",
    "    \n",
    "else:\n",
    "    test = x_test[0:50]\n",
    "    labels = y_test[0:50]\n",
    "    \n",
    "# print(labels)\n",
    "n_classes=len(nb_labels)\n",
    "print(n_classes)\n",
    "sal = np.empty((test.shape[0], test.shape[1],n_classes))   \n",
    "# sal = np.empty((test.shape[0], test.shape[1],len(nb_labels)))   \n",
    "for x, test_input in enumerate(test):\n",
    "    \n",
    "    samples = gen_initial_mask(N, test.shape[1], np.mean(test),np.std(test))\n",
    "    sample = samples[0]\n",
    "    samples = samples.reshape(samples.shape[0], samples.shape[1],1)\n",
    "    \n",
    "    test_input = (test_input - np.mean(test_input))/np.std(test_input)\n",
    "#     print('Data after normalization',test_input.shape)\n",
    "    masked_inputs = samples*test_input + samples\n",
    "    \n",
    "    masked_inputs = (masked_inputs - np.mean(masked_inputs))/np.std(masked_inputs)\n",
    "#     print('samples', masked_inputs.shape)\n",
    "    \n",
    "    true_pred = model.predict(test_input.reshape(1, test.shape[1],1))\n",
    "    true_preds = np.tile(true_pred, (N,1))\n",
    "    class_idx = np.argmax(true_pred)\n",
    "    \n",
    "    classes = np.unique(np.concatenate((y_train, y_test), axis=0))\n",
    "    masked_preds = model.predict(masked_inputs)\n",
    "    plt.figure()\n",
    "    for inp in masked_inputs[0:30]:\n",
    "        plt.plot(inp)\n",
    "    plt.show()\n",
    "    test_inputs = np.tile(test_input[:,0], (N, 1))\n",
    "#     test_inputs = test_inputs.reshape(N, test.shape[1],1)\n",
    "#     print(masked_inputs.shape)\n",
    "    recon = autoencoder.predict(masked_inputs)\n",
    "    recon_preds = model.predict(recon.reshape(N, test.shape[1], 1))\n",
    "    print(recon_preds.shape, masked_inputs.shape, recon.shape, test_inputs.shape)\n",
    "    mse = calc_mse(recon, samples, masked_inputs[:,:,0], recon_preds, masked_preds, test_inputs, class_idx, 0.005)\n",
    "#     sal[x] = recon_preds[:,class_idx].T.dot(mse*masked_inputs.reshape(masked_inputs.shape[0], masked_inputs.shape[1])).reshape(test.shape[1],1)\n",
    "#     sal[x] = recon_preds[:,class_idx].T.dot(mse).reshape(test.shape[1],1)\n",
    "    sal[x] = mse.reshape(test.shape[1],n_classes)\n",
    "    sal[x]=mse.T\n",
    "    print(sal[x].shape)\n",
    "#     sal[x] = np.sum(mse*masked_inputs.reshape(masked_inputs.shape[0], masked_inputs.shape[1]), axis=0).reshape(sal.shape[1],1)\n",
    "#     sal[x] = np.sum((recon.reshape(recon.shape[0], recon.shape[1],1) - test_input)*(recon_preds[:,class_idx])*lamb)\n",
    "#     sal[x] = np.sum( (np.abs((samples - recon)))*lamb + recon*(1-lamb))\n",
    "    \n",
    "#     sal[x] = np.sum(mse)\n",
    "#         print(recon[i].shape, x_train[0].shape, mse.shape)\n",
    "    plt.figure()\n",
    "#     plt.plot(sal[x,:,0])\n",
    "#     plt.plot(sal[x,:,1])\n",
    "    \n",
    "    for s in sal[x].T:\n",
    "        plt.plot(np.abs(s))\n",
    "    plt.show()\n",
    "\n",
    "# np.save(\"/rhome/s7thakur/MIRROR/scores/UCI-\" + dataset +\"-MIRROR-SaliencyScore.npy\",sal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assign score using optimization \n",
    "import numpy as np\n",
    "# from autograd import grad\n",
    "from numpy import linalg as LA\n",
    "# from autograd import elementwise_grad as egrad\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import sklearn\n",
    "\n",
    "def norm(x):\n",
    "    \n",
    "    return  (x - np.mean(x))/np.std(x)\n",
    "\n",
    "def minmax(x):\n",
    "    \n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "\n",
    "def sigmoid(x):  \n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "    \n",
    "# bins = 10\n",
    "N=100\n",
    "R=20\n",
    "lamb1=0.9\n",
    "lamb2=0.999\n",
    "lr=1e-2\n",
    "ep=1e-8\n",
    "random_seed=0    \n",
    "np.random.seed(random_seed)\n",
    "\n",
    "\n",
    "if x_train.shape[0] < x_test.shape[0]:\n",
    "    test = x_train[0:50]\n",
    "    labels = y_train[0:50]\n",
    "    \n",
    "else:\n",
    "    test = x_test[0:50]\n",
    "    labels = y_test[0:50]\n",
    "    \n",
    "# print(labels)\n",
    "\n",
    "def cost(M):\n",
    "    \n",
    "    y = M*(np.sqrt(masked_pred[0,class_idx])**2)\n",
    "    \n",
    "    return y\n",
    "\n",
    "def scale(sal):\n",
    "#     print(sal.shape)\n",
    "    for i,sa in enumerate(sal):\n",
    "#         print(i, sa.shape)\n",
    "        minimum = np.min(sa)\n",
    "        sa = np.subtract(sa , minimum)\n",
    "        sa = np.divide(sa, np.max(sa, axis=0))\n",
    "        sal[i,:] = sa * 100\n",
    "#     print(sal.shape)\n",
    "    return sal\n",
    "\n",
    "\n",
    "n_classes = len(np.unique(nb_labels))\n",
    "# initialize moment and velocity vectors for the target class\n",
    "sal = np.ones((test.shape[0], test.shape[1],n_classes))      \n",
    "\n",
    "for x,test_input in enumerate(test):\n",
    "    \n",
    "    sal_itrs = np.empty((R, test.shape[1],n_classes))   \n",
    "    preds = np.empty((R, 1,n_classes))      \n",
    "    \n",
    "#     print(sal_itrs.shape)\n",
    "    for i in range(0,R):\n",
    "        \n",
    "\n",
    "\n",
    "#         M = gen_initial_mask(100, test.shape[1],0.5)\n",
    "        M=gen_initial_mask_binary(N,test.shape[1],p=0.6,window='flat')\n",
    "#         M = gen_initial_mask_binary(100, test.shape[1],0.5)\n",
    "        \n",
    "#         plt.figure()\n",
    "#         for inp in M[0:30]:\n",
    "#             plt.plot(inp)\n",
    "#         plt.show()\n",
    "        \n",
    "        M = M.reshape(M.shape[0], M.shape[1],1)\n",
    "        M = minmax(M)\n",
    "        test_input = minmax(test_input)\n",
    "#         masked_inputs = M\n",
    "        masked_inputs = M*test_input\n",
    "        test_inputs = np.tile(test_input[:,0], (N, 1))\n",
    "        test_inputs = test_inputs.reshape(test_inputs.shape[0],test_inputs.shape[1],1)\n",
    "\n",
    "        true_preds = model.predict(test_inputs)\n",
    "        masked_preds = model.predict(masked_inputs)\n",
    "        unmasked_preds = model.predict(1-masked_inputs)\n",
    "        \n",
    "    #     mask_grads = np.gradient(masked_inputs[:,:,0][0:30],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    #     plt.figure()\n",
    "    #     for inp in mask_grads:\n",
    "    #         plt.plot(1-inp)\n",
    "    #     plt.show()\n",
    "    #     print('masked input shape:', masked_inputs[:,:,0][0:30].shape)\n",
    "        grad_on = ((np.gradient(norm(masked_inputs[:,:,0]),axis=1)))\n",
    "                        \n",
    "        grad_off = (np.abs(np.gradient(norm(1-masked_inputs[:,:,0]),axis=1)))\n",
    "        \n",
    "        plt.figure()\n",
    "        for inp in grad_on[0:30]:\n",
    "            plt.plot(inp)\n",
    "        plt.show()\n",
    "        \n",
    "        masked_preds = model.predict(grad_on)\n",
    "        unmasked_preds = model.predict(grad_off)\n",
    "        \n",
    "        M_grads_on= (masked_preds).T.dot((norm(grad_on)))*lr  \n",
    "        M_grads_off= (1-masked_preds).T.dot((norm(grad_off)))*(lr)\n",
    "        M_grads_anti= (unmasked_preds).T.dot((norm(grad_off)))*(lr)\n",
    "        \n",
    "#         + ep*LA.norm(np.abs(np.gradient(norm(masked_inputs[:,:,0]),axis=1)))\n",
    "                           \n",
    "#         + np.square(1-masked_preds).T.dot(norm(np.abs(np.gradient((1-masked_inputs[:,:,0]),axis=1))))*(1-lr)\n",
    "    #     + np.square(1-masked_preds).T.dot(norm(np.abs(np.gradient((1-masked_inputs[:,:,0]),axis=1))))*(1-lr)\n",
    "    #     M_grads= np.gradient(masked_preds.T.dot(norm(masked_inputs[:,:,0])),axis=1)*lr + np.gradient((1-masked_preds).T.dot(norm((1-masked_inputs[:,:,0]))),axis=1)*(1-lr)\n",
    "    #     M_grads= np.square(np.gradient(np.square(masked_preds).T.dot(norm(masked_inputs[:,:,0])),axis=1))*lr  + np.square(np.gradient(np.square(1-masked_preds).T.dot(norm((1-masked_inputs[:,:,0]))),axis=1))*(1-lr)\n",
    "\n",
    "\n",
    "#         dists = np.square((masked_preds).T.dot(((norm(masked_inputs[:,:,0])-norm(test_inputs[:,:,0])))))\n",
    "#         dists = np.einsum(\"ij, jk->ik\", M_grads_preds, norm(M_grads))\n",
    "#         dists = M_grads.T.dot(M_grads_preds)\n",
    "        temp_on=((M_grads_on))\n",
    "        temp_off=((M_grads_off))\n",
    "        unmasked = ((M_grads_anti))\n",
    "#         print(unmasked.shape, temp_on.shape)\n",
    "        print(np.mean(masked_preds,axis=0),np.mean(masked_preds,axis=0).shape)\n",
    "    \n",
    "        sal_itrs[i] = (((temp_on)).T)/N        \n",
    "        \n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(test_input,c='black')\n",
    "        for sa in temp_on:\n",
    "            plt.plot(norm(sa))\n",
    "        plt.legend(np.concatenate((['x'],nb_labels),axis=None))\n",
    "\n",
    "        plt.title('ON gradient with confidence score, Predicted Class = {},iteration {}'.format(np.argmax(true_preds[0]), i))\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(test_input,c='black')\n",
    "        for sa in temp_off:\n",
    "            plt.plot(norm(sa))\n",
    "        plt.legend(np.concatenate((['x'],nb_labels),axis=None))\n",
    "\n",
    "        plt.title('OFF, Predicted Class = {},iteration {}'.format(np.argmax(true_preds[0]), i))\n",
    "        plt.show()\n",
    "        \n",
    "#         plt.figure()\n",
    "#         plt.plot(test_input,c='black')\n",
    "#         for sa in unmasked:\n",
    "#             plt.plot(norm(sa))\n",
    "#         plt.legend(np.concatenate((['x'],nb_labels),axis=None))\n",
    "\n",
    "#         plt.title('Unmasked, Predicted Class = {},iteration {}'.format(np.argmax(true_preds[0]), i))\n",
    "#         plt.show()\n",
    "        \n",
    "#         plt.figure()\n",
    "#         plt.plot(test_input,c='black')\n",
    "#         for sa in sal_itrs[i].T:\n",
    "#             plt.plot((sa))\n",
    "#         plt.legend(np.concatenate((['x'],nb_labels),axis=None))\n",
    "\n",
    "#         plt.title('Saliency iteration, Predicted Class = {},iteration {}'.format(np.argmax(true_preds[0]), i))\n",
    "#         plt.show()\n",
    "        \n",
    "            \n",
    "    print('sal_itrs shape',sal_itrs.shape)\n",
    "    \n",
    "#     temp = np.zeros((sal_itrs.shape[1],sal_itrs.shape[2]))\n",
    "    \n",
    "#     for sa in sal_itrs:\n",
    "#         print('iterations')\n",
    "# #         print(sa.shape)\n",
    "        \n",
    "#         print(sa.shape)\n",
    "#         temp = temp + norm(sa)\n",
    "#         print(temp.shape)\n",
    "        \n",
    "#         plt.figure()\n",
    "#         for s in temp.T:\n",
    "#             print('class specific sal')\n",
    "#             plt.plot(norm(s))\n",
    "\n",
    "#         plt.show()\n",
    "        \n",
    "#     print(temp)\n",
    "#     print(np.sum(norm(np.abs(np.square(np.sum(sal_itrs, axis=0)))),axis=0).shape)\n",
    "    temp = np.einsum('ijk->jk',sal_itrs)\n",
    "#     for t in sal_itrs:\n",
    "#         sal[x] = sal[x]+scale(t)\n",
    "#     s=model.predict(temp.reshape(temp.shape[1],temp.shape[0],1))\n",
    "    \n",
    "    \n",
    "#     temp = np.einsum('ij,ik->ikj',s, temp.T)\n",
    "#     temp = np.einsum('ikj->ik',temp)\n",
    "    \n",
    "    \n",
    "#     print(temp.shape)\n",
    "    sal[x] = np.abs(temp)/R\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(test_input,c='black')\n",
    "    for sa in sal[x].T:\n",
    "        plt.plot((norm(sa)))\n",
    "    plt.legend(np.concatenate((['x'],nb_labels),axis=None))\n",
    "    \n",
    "    plt.title('Average Saliency, Predicted Class = {}'.format(np.argmax(true_preds[0])))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filter masks with low confidence before averaging\n",
    "\n",
    "# assign score using optimization \n",
    "import numpy as np\n",
    "# from autograd import grad\n",
    "from numpy import linalg as LA\n",
    "# from autograd import elementwise_grad as egrad\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import sklearn\n",
    "\n",
    "def norm(x):\n",
    "    \n",
    "    return  (x - np.mean(x))/np.std(x)\n",
    "\n",
    "def minmax(x):\n",
    "    \n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "\n",
    "def sigmoid(x):  \n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "    \n",
    "# bins = 10\n",
    "N=100\n",
    "R=20\n",
    "lamb1=0.9\n",
    "lamb2=0.999\n",
    "lr=1e-2\n",
    "ep=1e-8\n",
    "random_seed=0    \n",
    "np.random.seed(random_seed)\n",
    "\n",
    "\n",
    "if x_train.shape[0] < x_test.shape[0]:\n",
    "    test = x_train[0:50]\n",
    "    labels = y_train[0:50]\n",
    "    \n",
    "else:\n",
    "    test = x_test[0:50]\n",
    "    labels = y_test[0:50]\n",
    "    \n",
    "# print(labels)\n",
    "\n",
    "def cost(M):\n",
    "    \n",
    "    y = M*(np.sqrt(masked_pred[0,class_idx])**2)\n",
    "    \n",
    "    return y\n",
    "\n",
    "def scale(sal):\n",
    "#     print(sal.shape)\n",
    "    for i,sa in enumerate(sal):\n",
    "#         print(i, sa.shape)\n",
    "        minimum = np.min(sa)\n",
    "        sa = np.subtract(sa , minimum)\n",
    "        sa = np.divide(sa, np.max(sa, axis=0))\n",
    "        sal[i,:] = sa * 100\n",
    "#     print(sal.shape)\n",
    "    return sal\n",
    "\n",
    "\n",
    "n_classes = len(np.unique(nb_labels))\n",
    "# initialize moment and velocity vectors for the target class\n",
    "sal = np.ones((test.shape[0], test.shape[1],n_classes))      \n",
    "\n",
    "for x,test_input in enumerate(test):\n",
    "    \n",
    "    sal_itrs = np.empty((R, test.shape[1],n_classes))   \n",
    "    preds = np.empty((R, 1,n_classes))      \n",
    "    \n",
    "#     print(sal_itrs.shape)\n",
    "    for i in range(0,R):\n",
    "        \n",
    "\n",
    "\n",
    "#         M = gen_initial_mask(100, test.shape[1],0.5)\n",
    "        M=gen_initial_mask_binary(N,test.shape[1],p=0.6,window='flat')\n",
    "#         M = gen_initial_mask_binary(100, test.shape[1],0.5)\n",
    "        \n",
    "#         plt.figure()\n",
    "#         for inp in M[0:30]:\n",
    "#             plt.plot(inp)\n",
    "#         plt.show()\n",
    "        \n",
    "        M = M.reshape(M.shape[0], M.shape[1],1)\n",
    "        M = minmax(M)\n",
    "        test_input = minmax(test_input)\n",
    "#         masked_inputs = M\n",
    "        masked_inputs = M*test_input\n",
    "#         test_inputs = np.tile(test_input[:,0], (N, 1))\n",
    "#         test_inputs = test_inputs.reshape(test_inputs.shape[0],test_inputs.shape[1],1)\n",
    "\n",
    "        true_preds = model.predict(test_input.reshape(1,test_input.shape[0],1))\n",
    "        masked_preds = model.predict(masked_inputs)\n",
    "        unmasked_preds = model.predict(1-masked_inputs)\n",
    "        true_class = np.argmax(true_preds)\n",
    "        \n",
    "    #     mask_grads = np.gradient(masked_inputs[:,:,0][0:30],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    #     plt.figure()\n",
    "    #     for inp in mask_grads:\n",
    "    #         plt.plot(1-inp)\n",
    "    #     plt.show()\n",
    "    #     print('masked input shape:', masked_inputs[:,:,0][0:30].shape)\n",
    "        grad_on = (np.abs(np.gradient(norm(masked_inputs[:,:,0]),axis=1)))\n",
    "                        \n",
    "        grad_off = (np.abs(np.gradient(norm(1-masked_inputs[:,:,0]),axis=1)))\n",
    "        \n",
    "        plt.figure()\n",
    "        for inp in grad_on[0:30]:\n",
    "            plt.plot(inp)\n",
    "        plt.show()\n",
    "        masked_preds = model.predict(grad_on)\n",
    "        unmasked_preds = model.predict(grad_off)\n",
    "        # check confidence scores\n",
    "        include = np.where(masked_preds[:,true_class] > 0.7)\n",
    "        print(include)\n",
    "        masked_preds = masked_preds[include]\n",
    "        print(true_preds,'pred class', true_class, 'true class',labels[x])\n",
    "        \n",
    "        grad_on = grad_on[include]\n",
    "#         grad_off = grad_off[include]\n",
    "        # filter masks with low confidence scores\n",
    "        \n",
    "        # multiply the masks with confidence scores and sum over the masks to get a final saliency \n",
    "        M_grads_on= np.square(masked_preds).T.dot((minmax(grad_on)))*lr  \n",
    "#         M_grads_off= (1-masked_preds).T.dot((norm(grad_off)))*(lr)\n",
    "        M_grads_anti= (unmasked_preds).T.dot((norm(grad_off)))*(lr)\n",
    "        \n",
    "\n",
    "        temp_on=((M_grads_on))\n",
    "#         temp_off=((M_grads_off))\n",
    "        unmasked = ((M_grads_anti))\n",
    "    \n",
    "        sal_itrs[i] = (((temp_on)).T)/N        \n",
    "        \n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(test_input,c='black')\n",
    "        for sa in temp_on:\n",
    "            plt.plot(norm(sa))\n",
    "        plt.legend(np.concatenate((['x'],nb_labels),axis=None))\n",
    "\n",
    "        plt.title('ON gradient with confidence score, Predicted Class = {},iteration {}'.format(np.argmax(true_preds[0]), i))\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(test_input,c='black')\n",
    "        for sa in temp_off:\n",
    "            plt.plot(norm(sa))\n",
    "        plt.legend(np.concatenate((['x'],nb_labels),axis=None))\n",
    "\n",
    "        plt.title('OFF, Predicted Class = {},iteration {}'.format(np.argmax(true_preds[0]), i))\n",
    "        plt.show()\n",
    "        \n",
    "#         plt.figure()\n",
    "#         plt.plot(test_input,c='black')\n",
    "#         for sa in unmasked:\n",
    "#             plt.plot(norm(sa))\n",
    "#         plt.legend(np.concatenate((['x'],nb_labels),axis=None))\n",
    "\n",
    "#         plt.title('Unmasked, Predicted Class = {},iteration {}'.format(np.argmax(true_preds[0]), i))\n",
    "#         plt.show()\n",
    "        \n",
    "#         plt.figure()\n",
    "#         plt.plot(test_input,c='black')\n",
    "#         for sa in sal_itrs[i].T:\n",
    "#             plt.plot((sa))\n",
    "#         plt.legend(np.concatenate((['x'],nb_labels),axis=None))\n",
    "\n",
    "#         plt.title('Saliency iteration, Predicted Class = {},iteration {}'.format(np.argmax(true_preds[0]), i))\n",
    "#         plt.show()\n",
    "        \n",
    "            \n",
    "    print('sal_itrs shape',sal_itrs.shape)\n",
    "    \n",
    "#     temp = np.zeros((sal_itrs.shape[1],sal_itrs.shape[2]))\n",
    "    \n",
    "#     for sa in sal_itrs:\n",
    "#         print('iterations')\n",
    "# #         print(sa.shape)\n",
    "        \n",
    "#         print(sa.shape)\n",
    "#         temp = temp + norm(sa)\n",
    "#         print(temp.shape)\n",
    "        \n",
    "#         plt.figure()\n",
    "#         for s in temp.T:\n",
    "#             print('class specific sal')\n",
    "#             plt.plot(norm(s))\n",
    "\n",
    "#         plt.show()\n",
    "        \n",
    "#     print(temp)\n",
    "#     print(np.sum(norm(np.abs(np.square(np.sum(sal_itrs, axis=0)))),axis=0).shape)\n",
    "    temp = np.einsum('ijk->jk',sal_itrs)\n",
    "\n",
    "    sal[x] = np.abs(temp)/R\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(test_input,c='black')\n",
    "    for sa in sal[x].T:\n",
    "        plt.plot((norm(sa)))\n",
    "    plt.legend(np.concatenate((['x'],nb_labels),axis=None))\n",
    "    \n",
    "    plt.title('Average Saliency, Predicted Class = {}'.format(np.argmax(true_preds[0])))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filter masks with low confidence before averaging\n",
    "\n",
    "# assign score using optimization \n",
    "import numpy as np\n",
    "# from autograd import grad\n",
    "from numpy import linalg as LA\n",
    "# from autograd import elementwise_grad as egrad\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "# import plotly.io as pio\n",
    "# pio.renderers.default=\"jupyterlab\"\n",
    "# import plotly.figure_factory as ff\n",
    "def norm(x):\n",
    "    \n",
    "    return  (x - np.mean(x))/np.std(x)\n",
    "\n",
    "def minmax(x):\n",
    "    \n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "\n",
    "def sigmoid(x):  \n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "    \n",
    "# bins = 10\n",
    "N=100\n",
    "R=20\n",
    "lamb1=0.9\n",
    "lamb2=0.999\n",
    "lr=1e-2\n",
    "ep=1e-8\n",
    "random_seed=0    \n",
    "np.random.seed(random_seed)\n",
    "\n",
    "\n",
    "if x_train.shape[0] < x_test.shape[0]:\n",
    "    test = x_train[0:50]\n",
    "    labels = y_train[0:50]\n",
    "    \n",
    "else:\n",
    "    test = x_test[0:50]\n",
    "    labels = y_test[0:50]\n",
    "    \n",
    "# print(labels)\n",
    "\n",
    "def cost(M):\n",
    "    \n",
    "    y = M*(np.sqrt(masked_pred[0,class_idx])**2)\n",
    "    \n",
    "    return y\n",
    "\n",
    "def scale(sal):\n",
    "#     print(sal.shape)\n",
    "    for i,sa in enumerate(sal):\n",
    "#         print(i, sa.shape)\n",
    "        minimum = np.min(sa)\n",
    "        sa = np.subtract(sa , minimum)\n",
    "        sa = np.divide(sa, np.max(sa, axis=0))\n",
    "        sal[i,:] = sa * 100\n",
    "#     print(sal.shape)\n",
    "    return sal\n",
    "\n",
    "def plot_score_densities(preds):\n",
    "    preds = norm(preds)\n",
    "    df = pd.DataFrame(preds, columns=list(map(str, np.arange(0,preds.shape[1]))))\n",
    "    sns.displot(df,kind='ecdf')\n",
    "    plt.show()\n",
    "#     fig =  ff.create_distplot([df[c] for c in df.columns],df.columns, bin_size=0.25)\n",
    "#     fig.update_layout(title_text='Density plot of confidence scores')\n",
    "#     fig.show()\n",
    "    \n",
    "            \n",
    "\n",
    "n_classes = len(np.unique(nb_labels))\n",
    "print(n_classes)\n",
    "# initialize moment and velocity vectors for the target class\n",
    "sal = np.ones((test.shape[0], test.shape[1],n_classes))      \n",
    "masked_preds_inputs = np.empty((test.shape[0],N,n_classes))\n",
    "for x,test_input in enumerate(test):\n",
    "    j=0\n",
    "    \n",
    "    sal_itrs = np.empty((R, test.shape[1],n_classes))   \n",
    "    preds = np.empty((R, 1,n_classes))      \n",
    "    temp_masked_preds = np.empty((R, N, n_classes))\n",
    "#     print(sal_itrs.shape)\n",
    "    for i in range(0,R):\n",
    "        \n",
    "        M=gen_initial_mask_binary(N,test.shape[1],p=0.6, max_lobes=3,winsize=20,window='flat')\n",
    "        M_noise=gen_initial_mask(N,test.shape[1],p=0.6)\n",
    "        \n",
    "        M = M.reshape(M.shape[0], M.shape[1],1)\n",
    "        M_noise = M_noise.reshape(M_noise.shape[0], M_noise.shape[1],1)\n",
    "        \n",
    "        M = norm(M)\n",
    "        M = M*M_noise\n",
    "        \n",
    "        test_input = norm(test_input)\n",
    "        print(M.shape, test_input.shape)\n",
    "        masked_inputs = norm(M*test_input)\n",
    "\n",
    "        true_preds = model.predict(test_input.reshape(1,test_input.shape[0],1))\n",
    "        masked_preds = model.predict(masked_inputs)\n",
    "        unmasked_preds = model.predict(1-masked_inputs)\n",
    "        true_class = np.argmax(true_preds)\n",
    "        plt.figure()\n",
    "        for c in np.unique(nb_labels):\n",
    "            print(masked_preds.shape)\n",
    "            plt.plot(masked_preds[:,c])\n",
    "        plt.legend(nb_labels)\n",
    "        plt.show()\n",
    "        \n",
    "        plot_score_densities(masked_preds)\n",
    "        \n",
    "        print('true class',labels[x],'pred class',true_class, 'conf scores',true_preds)\n",
    "        print('masked pred',np.argmax(masked_preds[2]))\n",
    "\n",
    "\n",
    "        grad_on = ((np.gradient(norm(masked_inputs[:,:,0]),axis=1)))\n",
    "                        \n",
    "        grad_off = ((np.gradient(norm(1-masked_inputs[:,:,0]),axis=1)))\n",
    "        \n",
    "        plt.figure()\n",
    "        for inp in masked_inputs[0:30]:\n",
    "            plt.plot(inp)\n",
    "        plt.show()\n",
    "#         masked_preds = model.predict(grad_on)\n",
    "#         unmasked_preds = model.predict(grad_off)\n",
    "#         plot_score_densities(masked_preds)\n",
    "#         check confidence scores\n",
    "#         include = np.where(masked_preds[:,true_class] > 0.5)\n",
    "#         print(include)\n",
    "#         print(masked_preds)\n",
    "#         masked_preds = masked_preds[include]\n",
    "#         print(true_preds,'pred class', true_class, 'true class',labels[x],'masked pred',np.argmax(masked_preds,axis=1))\n",
    "        \n",
    "#         filter masks with low confidence scores\n",
    "#         grad_on = grad_on[include]\n",
    "        \n",
    "#         multiply the masks with confidence scores and sum over the masks to get a final saliency \n",
    "        M_grads_on= (masked_preds).T.dot(((M[:,:,0])))*lr  \n",
    "        M_grads_anti= (unmasked_preds).T.dot(((M[:,:,0])))*(lr)\n",
    "        \n",
    "\n",
    "        temp_on=((M_grads_on))\n",
    "        temp_off = ((M_grads_anti))\n",
    "        temp_masked_preds[i]=masked_preds\n",
    "        sal_itrs[i] = (((temp_on)).T)/N       \n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(test_input,c='black')\n",
    "        for sa in temp_on:\n",
    "            plt.plot(norm(sa))\n",
    "        plt.legend(np.concatenate((['x'],nb_labels),axis=None))\n",
    "\n",
    "        plt.title('ON gradient with confidence score, Predicted Class = {},iteration {}'.format(np.argmax(true_preds[0]), i))\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(test_input,c='black')\n",
    "        for sa in temp_off:\n",
    "            plt.plot(norm(sa))\n",
    "        plt.legend(np.concatenate((['x'],nb_labels),axis=None))\n",
    "\n",
    "        plt.title('OFF, Predicted Class = {},iteration {}'.format(np.argmax(true_preds[0]), i))\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "            \n",
    "#     print('sal_itrs shape',sal_itrs.shape)\n",
    "    \n",
    "\n",
    "    temp = np.einsum('ijk->jk',sal_itrs)\n",
    "    \n",
    "    print(np.mean(temp_masked_preds, axis=0).shape)\n",
    "    masked_preds_inputs[x]=np.mean(temp_masked_preds, axis=0)\n",
    "    sal[x] = (temp/R)\n",
    "    j=j+1\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(test_input,c='black')\n",
    "    for sa in sal[x].T:\n",
    "        plt.plot((norm(sa)))\n",
    "    plt.legend(np.concatenate((['x'],nb_labels),axis=None))\n",
    "    \n",
    "    plt.title('Average Saliency, Predicted Class = {}'.format(np.argmax(true_preds[0])))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = np.unique(Y_train)\n",
    "\n",
    "classes = labels\n",
    "\n",
    "# N = X_test.shape[0]\n",
    "N=len(dataloader_test)\n",
    "# bins = 10\n",
    "batch=300\n",
    "epochs=20\n",
    "lamb1=0.9\n",
    "lamb2=0.999\n",
    "lr=1e-2\n",
    "ep=1e-8\n",
    "random_seed=0    \n",
    "np.random.seed(random_seed)\n",
    "\n",
    "if len(input_shape) == 2:\n",
    "\n",
    "    ndim = input_shape[0] if input_shape[1]<input_shape[0] else input_shape[1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# if int((6/10)*self.ndim) <= self.window_size: return\n",
    "# print(int((6/10)*self.ndim) , self.window_size)\n",
    "\n",
    "n_classes = len(classes)\n",
    "\n",
    "sal = np.ones((N, ndim, n_classes))      \n",
    "print(sal.shape)\n",
    "# total_preds = np.empty((self.N, self.epochs, self.batch, n_classes))\n",
    "# total_true_preds = np.empty((self.N, self.epochs, self.batch, n_classes))\n",
    "\n",
    "\n",
    "# print(self.inp_shape, len(self.test))\n",
    "\n",
    "test_acc = 0.0\n",
    "for x, row in enumerate(dataloader_test):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        sample, label = row\n",
    "        sample, label = sample.cuda(), label.cuda()\n",
    "\n",
    "#         start = time.time()\n",
    "\n",
    "        sal_itrs = np.empty((epochs, ndim, n_classes))   \n",
    "#         print(sal_itrs.shape)\n",
    "#         preds = np.empty((self.epochs, 1,n_classes))      \n",
    "#         temp_masked_preds = np.empty((self.epochs, self.batch, n_classes))\n",
    "#         temp_true_preds = np.empty((self.epochs, self.batch, n_classes))\n",
    "        # print(sal_itrs.shape, temp_masked_preds.shape)\n",
    "        j=0\n",
    "        for i in range(0, epochs):\n",
    "\n",
    "\n",
    "            M=gen_initial_mask_binary(batch,ndim,p=0.6, max_lobes=3,winsize=20,window='flat')\n",
    "            M_noise=gen_initial_mask(batch,ndim,p=0.6)\n",
    "#             print(M.shape, M_noise.shape)\n",
    "#             M=gen_random_binary_mask_samples()\n",
    "#             M_noise=get_gaussian_noise_samples()\n",
    "\n",
    "\n",
    "            M = M.reshape(batch, *input_shape)\n",
    "            M_noise = M_noise.reshape(batch, *input_shape)\n",
    "\n",
    "            M = norm(M)\n",
    "            M = M*M_noise\n",
    "            M = torch.from_numpy(M).type(torch.cuda.FloatTensor).cuda()\n",
    "            \n",
    "            # print(M.shape)\n",
    "            # print(M)\n",
    "            # test_input = norm(samples)\n",
    "            # print(sample.cpu().numpy())\n",
    "            masked_inputs = torch_norm(M*sample)\n",
    "            \n",
    "            # print(masked_inputs)\n",
    "            # masked_inputs = masked_inputs\n",
    "            # print(masked_inputs.shape)\n",
    "\n",
    "            # print(masked_inputs)\n",
    "\n",
    "\n",
    "            # print(samples.shape, labels.shape)\n",
    "            true_preds = model(sample)\n",
    "            # calculate accuracy\n",
    "\n",
    "            masked_preds = model(masked_inputs)\n",
    "            unmasked_preds = model(1-masked_inputs)\n",
    "\n",
    "            # print(masked_preds.size(), M[:,0,:].size())\n",
    "            M_grads_on= torch.mul(torch.matmul(masked_preds.T, M[:,0,:]),torch.tensor(lr).cuda())\n",
    "            M_grads_anti= torch.mul(torch.matmul(unmasked_preds.T, M[:,0,:]),torch.tensor(lr).cuda())\n",
    "            # print(M_grads_on.size())\n",
    "            true_preds = true_preds.cpu().numpy()\n",
    "            masked_preds = masked_preds.cpu().numpy()\n",
    "            M_grads_on = M_grads_on.cpu().numpy()\n",
    "            M_grads_anti = M_grads_anti.cpu().numpy()\n",
    "\n",
    "            temp_on= M_grads_on\n",
    "            temp_off = M_grads_anti\n",
    "#             temp_masked_preds[i]=masked_preds\n",
    "#             temp_true_preds[i]=true_preds\n",
    "            sal_itrs[i] = (temp_on.T)/batch    \n",
    "            # print(sal_itrs.shape)\n",
    "\n",
    "\n",
    "        temp = np.einsum('ijk->jk',sal_itrs)\n",
    "        # print(temp.shape)\n",
    "\n",
    "        # print(np.mean(temp_masked_preds, axis=0).shape)\n",
    "\n",
    "        # masked_preds_inputs[x]=np.mean(temp_masked_preds, axis=0)\n",
    "        sal[x] = (temp/epochs)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(sample.cpu().numpy()[0][0].T,c='black')\n",
    "        for sa in sal[x].T:\n",
    "            plt.plot((norm(sa)))\n",
    "        plt.legend(np.concatenate((['x'],classes),axis=None))\n",
    "\n",
    "        plt.title('Average Saliency')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assign score using optimization \n",
    "import numpy as np\n",
    "# from autograd import grad\n",
    "from numpy import linalg as LA\n",
    "# from autograd import elementwise_grad as egrad\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import sklearn\n",
    "\n",
    "def norm(x):\n",
    "    \n",
    "    return  (x - np.mean(x))/np.std(x)\n",
    "\n",
    "def minmax(x):\n",
    "    \n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "\n",
    "def sigmoid(x):  \n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "    \n",
    "# bins = 10\n",
    "N=500\n",
    "lamb1=0.9\n",
    "lamb2=0.999\n",
    "lr=1e-2\n",
    "ep=1e-8\n",
    "random_seed=0    \n",
    "np.random.seed(random_seed)\n",
    "\n",
    "\n",
    "if x_train.shape[0] < x_test.shape[0]:\n",
    "    test = x_train[0:50]\n",
    "    labels = y_train[0:50]\n",
    "    \n",
    "else:\n",
    "    test = x_test[0:50]\n",
    "    labels = y_test[0:50]\n",
    "    \n",
    "# print(labels)\n",
    "\n",
    "def cost( pred):\n",
    "    \n",
    "    y = (np.sqrt(pred)**2)\n",
    "    \n",
    "    return y\n",
    "\n",
    "def scale(sal):\n",
    "    \n",
    "    minimum = np.min(sal, axis=1)\n",
    "    sal = np.subtract(sal , minimum)\n",
    "    sal = np.divide(sal, np.max(sal, axis=1))\n",
    "    sal = sal * 100\n",
    "    return sal\n",
    "\n",
    "\n",
    "n_classes = len(np.unique(nb_labels))\n",
    "# initialize moment and velocity vectors for the target class\n",
    "sal = np.empty((test.shape[0], test.shape[1],n_classes))      \n",
    "\n",
    "for x,test_input in enumerate(test):\n",
    "    \n",
    "    \n",
    "\n",
    "        print(test_input.shape)\n",
    "        test_input = norm(test_input)\n",
    "        \n",
    "        test_input = test_input.reshape(1,test_input.shape[0],1)\n",
    "\n",
    "        true_pred = model.predict(test_input)\n",
    "        true_class = np.argmax(true_pred)\n",
    "        print('true class',true_class)\n",
    "        M = gen_initial_mask(1, test_input.shape[1],0.5)\n",
    "        M = M.reshape(test_input.shape)\n",
    "        \n",
    "        M = M*(test_input)+M**2\n",
    "        M = norm(M)\n",
    "        plt.figure()\n",
    "        plt.plot(M[0])\n",
    "        plt.show()\n",
    "        print(M.shape)\n",
    "        masked_pred = model.predict(M)\n",
    "        \n",
    "        p_cur = masked_pred[0, true_class]\n",
    "        p_prev = 0.0\n",
    "        p_delta = 0.0\n",
    "        sal_prev = M\n",
    "        for i in range(0,N):\n",
    "        \n",
    "            \n",
    "            masked_pred = model.predict(sal_prev)\n",
    "            p_cur = masked_pred[0,true_class]\n",
    "            print('masked pred', p_cur)\n",
    "            \n",
    "            if i == 1:\n",
    "                \n",
    "                sal_cur= (p_cur)*((np.abs(np.gradient(norm(M),axis=1)))) *lr\n",
    "#                 + lr*LA.norm((np.abs(np.gradient(norm(M),axis=1))),'fro')  \n",
    "            else:\n",
    "                sal_cur= (p_cur)*((np.abs(np.gradient(norm(sal_prev),axis=1))))*(1-lr)  \n",
    "#                 + (1-lr)*LA.norm((np.abs(np.gradient(norm(sal_prev),axis=1))),'fro')\n",
    "                \n",
    "            print(sal_cur.shape)\n",
    "            if((p_cur - p_prev) > p_delta):\n",
    "                ids_pos = np.where(np.abs(sal_prev[0,:,0] - sal_cur[0,:,0])>0)    \n",
    "#                 print(ids_pos)\n",
    "                sal_cur[0,ids_pos,0] = sal_cur[0,ids_pos,0] + cost(p_cur)*lr\n",
    "\n",
    "            elif((p_cur-p_prev) <= p_delta ):\n",
    "                ids_neg = np.where(np.abs(sal_prev[0,:,0]-sal_cur[0,:,0])>0)\n",
    "                sal_cur[0,ids_neg,0] = sal_cur[0,ids_neg,0] - cost(p_cur)*(np.abs(sal_prev-sal_cur))[0,ids_neg,0]*lr\n",
    "\n",
    "\n",
    "#             elif(p_delta-ep <=(p_cur-p_prev) <= p_delta+ep):\n",
    "                \n",
    "\n",
    "            \n",
    "            p_prev = p_cur\n",
    "            sal_prev = norm(sal_cur)\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(test_input[0],c='black')\n",
    "            for sa in sal_prev.T:\n",
    "                plt.plot(norm(sa))\n",
    "            # add legend\n",
    "\n",
    "            plt.show()\n",
    "        sal[x] = (sal_prev)/N\n",
    "        \n",
    "#         plt.figure()\n",
    "#         plt.plot(test_input,c='black')\n",
    "#         for sa in sal[x].T:\n",
    "#             plt.plot(norm(sa))\n",
    "#         # add legend\n",
    "\n",
    "#         plt.title('Average Saliency, Predicted Class = {}'.format(np.argmax(true_preds[0])))\n",
    "#         plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if x_train.shape[0] < x_test.shape[0]:\n",
    "        test = x_train\n",
    "        labels = y_train\n",
    "\n",
    "# else:\n",
    "#     test = x_test[0:100]\n",
    "#     labels = y_test[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using clipping on masked inputs (not used), saliency using only masked inputs\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import sklearn\n",
    "\n",
    "def norm(x):\n",
    "    \n",
    "    return  (x - np.mean(x))/np.std(x)\n",
    "def minmax(x):\n",
    "    \n",
    "    return  (x - np.min(x))/(np.max(x)-np.min(x))\n",
    "\n",
    "def sigmoid(x):  \n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "    \n",
    "# bins = 10\n",
    "N = 2000\n",
    "\n",
    "def calc_mse( y, z, masked_preds, true_preds, test,  class_idx, lamb):\n",
    "#     grads_masked = np.gradient(z, axis=1)\n",
    "    grads_masked = np.gradient(((z)), axis=1)\n",
    "    mse1 = masked_preds.T.dot(grads_masked*lamb)\n",
    "    mse2 = (masked_preds-true_preds).T.dot((np.abs((norm(z)-norm(test)))))\n",
    "    \n",
    "    mse=((minmax(np.abs(mse1))*minmax(np.abs(mse2))))\n",
    "#     mse=((mse1))\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot((minmax(mse2).T),c='orange')\n",
    "    plt.plot((minmax(mse1).T),c='green')\n",
    "    plt.plot((minmax(mse).T),c='red')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "#     plt.figure()\n",
    "#     for inp in grads[0:20]:\n",
    "#         plt.plot(inp)\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "    return mse\n",
    "\n",
    "\n",
    "def explain(train,test,y_test,y_train, nb_labels, model):\n",
    "\n",
    "    \n",
    "\n",
    "    # print(labels)\n",
    "    n_classes=len(nb_labels)\n",
    "    sal = np.empty((test.shape[0], test.shape[1],n_classes))   \n",
    "    for x, test_input in enumerate(test):\n",
    "\n",
    "        samples = gen_initial_mask(N, test.shape[1], np.mean(test_input), np.std(test_input))\n",
    "        samples = samples.reshape(samples.shape[0], samples.shape[1],1)\n",
    "\n",
    "        masked_inputs = samples*test_input+samples\n",
    "\n",
    "        true_pred = model.predict(test_input.reshape(1, test.shape[1],1))\n",
    "        true_preds = np.tile(true_pred, (N,1))\n",
    "        class_idx = np.argmax(true_pred)\n",
    "\n",
    "        classes = np.unique(np.concatenate((y_train, y_test), axis=0))\n",
    "        masked_preds = model.predict(masked_inputs)\n",
    "        plt.figure()\n",
    "        for inp in masked_inputs[0:30]:\n",
    "            plt.plot(inp)\n",
    "        plt.show()\n",
    "        test_inputs = np.tile(test_input[:,0], (N, 1))\n",
    "    #     recon = autoencoder.predict(masked_inputs)\n",
    "    #     recon_preds = model.predict(recon.reshape(N, test.shape[1], 1))\n",
    "        mse = calc_mse( samples, masked_inputs[:,:,0], masked_preds, true_preds, test_inputs, class_idx, 0.005)\n",
    "        sal[x] = mse.T\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.plot(minmax(sal[x]))\n",
    "        plt.title('mse')\n",
    "        plt.show()\n",
    "        \n",
    "    np.save(\"/home/s7thakur/ecresearch-shared/compute1/MIRROR/scores/UCI-\" + dataset +\"-MIRROR-SaliencyScore-noreconstruction.npy\",sal)\n",
    "    return sal\n",
    "    # np.save(\"/rhome/s7thakur/MIRROR/scores/UCI-\" + dataset +\"-MIRROR-SaliencyScore-noreconstruction.npy\",sal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sal=explain(x_train, x_test, y_test, y_train,nb_labels, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def eval_insertion_score(test, sal, y_test, dataset, w=11,step=5, save=False,cbar=False):\n",
    "    X = test\n",
    "    Y = np.argmax(y_test, axis=1)\n",
    "#     labels=y_test\n",
    "    classes = np.unique(Y)\n",
    "    for c in classes:    \n",
    "        plt.figure()        \n",
    "        c_x = X[np.where(Y == int(c))]\n",
    "        c_sal = sal[np.where(Y==int(c))]\n",
    "        i=0\n",
    "        for sa, test_input in zip(c_sal, c_x):\n",
    "            print(sa[:,int(c)].shape)\n",
    "            compute_metric_score(test_input, (sa[:,int(c)]), dataset, i, w, step, save,'ins',cbar)\n",
    "            \n",
    "            i=i+1\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def eval_insertion_score_all(test, sal, y_test,  masked_preds_inputs, dataset, w=11,step=5, save=False,cbar=False):\n",
    "    X = test\n",
    "#     print(y_test)\n",
    "#     Y=y_test\n",
    "    Y = np.argmax(y_test, axis=1)\n",
    "#     labels=y_test\n",
    "\n",
    "    classes = np.unique(Y)\n",
    "#     print(classes)\n",
    "    for sa, test_input, masked_preds in zip(sal, test, masked_preds_inputs):\n",
    "        plt.figure()        \n",
    "        i=0\n",
    "        # compute ins score for all the classes, bar char of class frequencies, or violin plot\n",
    "        print('Next test input saliency map')\n",
    "        for c in classes:    \n",
    "#             print('class',c)\n",
    "            s_c = sa[:,int(c)]\n",
    "#             print(s_c.shape)\n",
    "            compute_metric_score(test_input,s_c, dataset, i, w, step, save,'ins',cbar, c)\n",
    "            \n",
    "        \n",
    "            i=i+1\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "#         df_preds = pd.DataFrame(np.concatenate([masked_preds_inputs, y_test], axis=0), columns=['Score','Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.argmax(masked_preds_inputs,axis=2)\n",
    "m,n = ids.shape[:2]\n",
    "I,J = np.ogrid[:m,:n]\n",
    "pred_labels= (ids).flatten()\n",
    "pred_scores = (masked_preds_inputs[I,J,ids]).flatten()\n",
    "inp_ids = [[i]*100 for i in I]\n",
    "inp_ids = np.array(inp_ids).flatten()\n",
    "true_labels=[[l]*100 for l in np.argmax(labels, axis=1)]\n",
    "true_labels = np.array(true_labels).flatten()\n",
    "\n",
    "df = pd.DataFrame(data=np.column_stack((inp_ids, pred_scores, pred_labels, true_labels)), columns=['Sample','Prediction Score','Prediction Label','True Label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# g = sns.catplot(x=\"Prediction Label\", y=\"total_bill\", kind=\"violin\", inner=None, data=df)\n",
    "# sns.swarmplot(x=\"Prediction Label\", y=\"Prediction Score\", color=\"k\", size=3, data=df, ax=g.ax)\n",
    "\n",
    "fig = px.histogram(df, x=\"Sample\", y=\"Prediction Score\", color=\"Prediction Label\",\n",
    "                   marginal=\"box\")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval_insertion_score_all(test, sal, labels,  masked_preds_inputs, dataset, 5, 5, False,False)\n",
    "eval_insertion_score(test, sal, labels, dataset, 5, 5, False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for different classifiers\n",
    "\n",
    "# for different datasets\n",
    "\n",
    "# report average insertion score (AUC score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal[np.where(labels==int(0))[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(Y_train[1500:1600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_set( x_test, y_test):\n",
    "    \n",
    "    test = x_test[1550:1600]\n",
    "    labels = y_test[1550:1600]\n",
    "        \n",
    "    return test,labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal=np.load('/home/s7thakur/ecresearch-shared/MIRROR/results/resnet/PhysioNet/mitdb/importance_score.npy', allow_pickle=True)\n",
    "# sal=sal.item()['importance_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sal = ((sal))\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "test,labels=get_test_set( X_train,Y_train)\n",
    "X = test\n",
    "# Y = np.argmax(labels,axis=1)\n",
    "Y = labels\n",
    "classes = np.unique(Y)\n",
    "print(classes)\n",
    "print(X.shape, Y.shape, sal.shape)\n",
    "# mse1 = mse1.reshape(mse1.shape[1], mse1.shape[0])\n",
    "ind=0\n",
    "# fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(12, 8))\n",
    "for c in classes:\n",
    "    \n",
    "#     plt.figure(figsize=(10,5))        \n",
    "    c_x = X[np.where(Y == c)]\n",
    "    c_sal = sal[np.where(Y==c)]\n",
    "    print(c_x.shape, c_sal.shape)\n",
    "    for sa, test_input in zip(c_sal, c_x):\n",
    "        sa = savgol_filter(sa[:,c], 5, 1) # window size 51, polynomial order 3\n",
    "        max_length = 6000\n",
    "        sa = norm(sa)\n",
    "        minimum = np.mean(sa)\n",
    "        \n",
    "        sa = sa - minimum\n",
    "        sa = sa / np.max(sa)\n",
    "        sa= sa * 100\n",
    "        \n",
    "        ts = test_input.reshape(1, test.shape[2],1)\n",
    "        x = np.linspace(0, ts.shape[1] - 1, max_length, endpoint=True)\n",
    "        f = interp1d(range(ts.shape[1]), ts[0, :, 0])\n",
    "        y = f(x)\n",
    "        \n",
    "        f = interp1d(range(ts.shape[1]), sa)\n",
    "        cas = f(x).astype(int)\n",
    "#         plt.plot(cas)\n",
    "\n",
    "        plt.scatter(x=x, y=y, c=cas, cmap='jet', marker='.', s=20, vmin=0, vmax=100, linewidths=0.0)\n",
    "        \n",
    "    plt.title('class:{}'.format(c), fontsize=20)\n",
    "        \n",
    "        \n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sal = ((sal))\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# test,labels=get_test_set(X_train,Y_train, X_test,Y_test)\n",
    "X =X_test\n",
    "# Y = labels\n",
    "# classes = np.unique(Y)\n",
    "Y= Y_test\n",
    "# print(classes)\n",
    "# print(X.shape, Y.shape, sal.shape)\n",
    "# mse1 = mse1.reshape(mse1.shape[1], mse1.shape[0])\n",
    "ind=0\n",
    "# fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(12, 8))\n",
    "for c in classes:\n",
    "    \n",
    "#     plt.figure(figsize=(10,5))        \n",
    "    c_x = X[np.where(Y == c)]\n",
    "    c_sal = sal[np.where(Y==c)]\n",
    "    print(c_x.shape, c_sal.shape)\n",
    "    for sa, test_input in zip(c_sal, c_x):\n",
    "        sa = savgol_filter(sa[:,c], 5, 1) # window size 51, polynomial order 3\n",
    "        max_length = 6000\n",
    "        sa = norm(sa)\n",
    "        minimum = np.mean(sa)\n",
    "        \n",
    "        sa = sa - minimum\n",
    "        sa = sa / np.max(sa)\n",
    "        sa= sa * 100\n",
    "        \n",
    "        ts = test_input.reshape(1, test.shape[1],1)\n",
    "        x = np.linspace(0, ts.shape[1] - 1, max_length, endpoint=True)\n",
    "        f = interp1d(range(ts.shape[1]), ts[0, :, 0])\n",
    "        y = f(x)\n",
    "        \n",
    "        f = interp1d(range(ts.shape[1]), sa)\n",
    "        cas = f(x).astype(int)\n",
    "#         plt.plot(cas)\n",
    "\n",
    "        plt.scatter(x=x, y=y, c=cas, cmap='jet', marker='.', s=20, vmin=0, vmax=100, linewidths=0.0)\n",
    "        \n",
    "    plt.title('class:{}'.format(c), fontsize=20)\n",
    "        \n",
    "        \n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# save saliency score visualization \n",
    "# sal = np.abs(sal)\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "X = test\n",
    "Y = np.argmax(labels,axis=1)\n",
    "# Y = labels\n",
    "classes = np.unique(Y)\n",
    "print(classes)\n",
    "print(X.shape, Y.shape, sal.shape)\n",
    "# mse1 = mse1.reshape(mse1.shape[1], mse1.shape[0])\n",
    "ind=0\n",
    "fig, axes = plt.subplots( ncols=len(classes), figsize=(20, 4))\n",
    "# fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for c,ax in zip(classes,axes):\n",
    "    \n",
    "#     plt.figure(figsize=(10,5))        \n",
    "    c_x = X[np.where(Y == c)]\n",
    "    c_sal = sal[np.where(Y==c)]\n",
    "    print(c_x.shape, c_sal.shape)\n",
    "    for sa, test_input in zip(c_sal, c_x):\n",
    "#         win = len(test_input_)/2\n",
    "        sa = savgol_filter(sa[:,c], 5, 1) # window size 51, polynomial order 3\n",
    "        max_length = 6000\n",
    "#         sa = np.abs(sa)\n",
    "        minimum = np.mean(sa)\n",
    "        \n",
    "        sa = sa - minimum\n",
    "        sa = sa / np.max(sa)\n",
    "        sa= sa * 100\n",
    "        \n",
    "        ts = test_input.reshape(1, test.shape[1],1)\n",
    "        x = np.linspace(0, ts.shape[1] - 1, max_length, endpoint=True)\n",
    "        f = interp1d(range(ts.shape[1]), ts[0, :, 0])\n",
    "        y = f(x)\n",
    "        \n",
    "        f = interp1d(range(ts.shape[1]), sa)\n",
    "        cas = f(x).astype(int)\n",
    "#         plt.plot(cas)\n",
    "\n",
    "#         plt.scatter(x=x, y=y, c=cas, cmap='jet', marker='.', s=20, vmin=0, vmax=100, linewidths=0.0)\n",
    "        \n",
    "#     plt.title('class:{}'.format(c), fontsize=20)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ax.scatter(x,y, c=cas, cmap='jet', marker='.', s=20, vmin=0, vmax=100, linewidths=0.0)\n",
    "    ax.set_title('class:{}'.format(c), size=25) \n",
    "    ax.set_ylabel('Saliency Score', size=25) #setting the ylabel and font size\n",
    "    ax.set_xlabel('Index', size=25)\n",
    "    ax.xaxis.set_tick_params(labelsize=25) #setting the font size of the x axis\n",
    "    ax.yaxis.set_tick_params(labelsize=25) #setting the font size of the y axis\n",
    "#         ind = ind+1\n",
    "    \n",
    "plt.tight_layout()\n",
    "# plt.savefig('/rhome/s7thakur/MIRROR/plots/UCI-'+dataset+'-MIRROR-SaliencyScore-run2.eps')\n",
    "# plt.savefig('/home/s7thakur/ecresearch-shared/compute1/MIRROR/plots/UCI-'+dataset+'-MIRROR-SaliencyScore-no-reconstruction-51.pdf',\n",
    "#            bbox_inches='tight',rasterized=True)\n",
    "# plt.savefig('/rhome/s7thakur/MIRROR/plots/UCI-'+dataset+'-MIRROR-SaliencyScore-run2.svg')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saliency for all the combination of classes and inputs\n",
    "\n",
    "# save saliency score visualization \n",
    "sal = (sal)\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "X = test\n",
    "# Y = np.argmax(labels,axis=1)\n",
    "Y = labels\n",
    "classes = np.unique(Y)\n",
    "print(classes)\n",
    "print(X.shape, Y.shape, sal.shape)\n",
    "# mse1 = mse1.reshape(mse1.shape[1], mse1.shape[0])\n",
    "ind=0\n",
    "fig, axes = plt.subplots(nrows=len(classes), ncols=len(classes), figsize=(25, 15))\n",
    "# fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for c_i in classes:\n",
    "    \n",
    "#     plt.figure(figsize=(10,5))   \n",
    "\n",
    "    c_x = X[np.where(Y == c_i)]\n",
    "    c_sal = sal[np.where(Y==c_i)]\n",
    "    print(c_i, c_x.shape, c_sal.shape)\n",
    "        \n",
    "    for c_j in classes:\n",
    "        for sa, test_input in zip(c_sal, c_x):\n",
    "#             print(sa.shape)\n",
    "    #         win = len(test_input_)/2\n",
    "            sa = savgol_filter(sa[:,c_j], 11, 1) # window size 51, polynomial order 3\n",
    "            max_length = 6000\n",
    "    #         sa = np.abs(sa)\n",
    "            minimum = np.mean(sa)\n",
    "        \n",
    "\n",
    "            sa = sa - minimum\n",
    "            sa = sa / np.max(sa)\n",
    "            sa= sa * 100\n",
    "\n",
    "            ts = test_input.reshape(1, test.shape[2],1)\n",
    "            x = np.linspace(0, ts.shape[1] - 1, max_length, endpoint=True)\n",
    "            f = interp1d(range(ts.shape[1]), ts[0, :, 0])\n",
    "            y = f(x)\n",
    "\n",
    "            f = interp1d(range(ts.shape[1]), sa)\n",
    "            cas = f(x).astype(int)\n",
    "    #         plt.plot(cas)\n",
    "\n",
    "    #         plt.scatter(x=x, y=y, c=cas, cmap='jet', marker='.', s=20, vmin=0, vmax=100, linewidths=0.0)\n",
    "\n",
    "    #     plt.title('class:{}'.format(c), fontsize=20)\n",
    "\n",
    "\n",
    "\n",
    "            axes[c_i,c_j].scatter(x,y, c=cas, cmap='jet', marker='.', s=20, vmin=0, vmax=100, linewidths=0.0)\n",
    "        axes[c_i,c_j].set_title('True Class:{}, Saliency Map Class:{}'.format(c_i,c_j), size=25) \n",
    "        axes[c_i,c_j].set_ylabel('Saliency Score', size=20) #setting the ylabel and font size\n",
    "        axes[c_i,c_j].set_xlabel('Time (ms)', size=20)\n",
    "        axes[c_i,c_j].xaxis.set_tick_params(labelsize=15) #setting the font size of the x axis\n",
    "        axes[c_i,c_j].yaxis.set_tick_params(labelsize=15) #setting the font size of the y axis\n",
    "    #         ind = ind+1\n",
    "   \n",
    "fig.tight_layout()\n",
    "# plt.savefig('/rhome/s7thakur/MIRROR/plots/UCI-'+dataset+'-MIRROR-SaliencyScore-run2.eps')\n",
    "# plt.savefig('/home/s7thakur/ecresearch-shared/MIRROR/plots/UCI-PhysioNet-0-1-MIRROR-class-discriminative-SaliencyScore.pdf')\n",
    "plt.savefig('/home/s7thakur/ecresearch-shared/MIRROR/plots/UCI-PhysioNet-0-1-MIRROR-class-discriminative-SaliencyScore.png',resolution=300)\n",
    "\n",
    "# plt.savefig('/rhome/s7thakur/MIRROR/plots/UCI-'+dataset+'-MIRROR-SaliencyScore-run2.svg')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(arr):\n",
    "    return (arr.sum() - arr[0]/2 - arr[-1]/2)/(arr.shape[0]-1)\n",
    "\n",
    "def compute_metric_score(test, sal, dataset, inp_n, w_s=11, step=5,save_to=False,mode='ins',cbar=False, given_clas='None'):\n",
    "    \n",
    "    \n",
    "    save_path = \"/home/s7thakur/ecresearch-shared/compute1/MIRROR/plots/\"\n",
    "    W = test.shape[0] # image area\n",
    "    n_classes = np.unique(labels)\n",
    "    mode = 'ins'\n",
    "    inp = test[:,0]\n",
    "#     sa = np.abs(sal)\n",
    "    sa = savgol_filter(sal, w_s, 1) # window size 51, polynomial order 3\n",
    "    minimum = np.mean(sa)\n",
    "\n",
    "    sa = sa - minimum\n",
    "    sa = sa / np.max(sa)\n",
    "    sa= sa * 100\n",
    "\n",
    "    explanation = sa\n",
    "#     fig, ax1 = plt.subplots()\n",
    "#     ax1.plot(inp)\n",
    "#     ax2 = ax1.twinx()\n",
    "#     ax2.plot(explanation, color='red')\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "    verbose = 1\n",
    "    \n",
    "    substrate = np.zeros(inp.shape)\n",
    "    \n",
    "    pred = model.predict(inp.reshape(1,W,1))\n",
    "    n_steps = (W + step - 1) // step\n",
    "    if mode == 'del':\n",
    "        title = 'Deletion game'\n",
    "        ylabel = 'Sample deleted'\n",
    "        start = inp\n",
    "        finish = substrate\n",
    "\n",
    "    elif mode == 'ins':\n",
    "        title = 'Insertion game'\n",
    "        ylabel = 'Sample inserted'\n",
    "        start = substrate\n",
    "        finish = inp\n",
    "\n",
    "    scores = np.empty(n_steps + 1)\n",
    "    clas = np.empty(n_steps + 1)\n",
    "    \n",
    "    salient_order = np.flip(np.argsort(explanation))\n",
    "    cl = np.argmax(pred)\n",
    "    \n",
    "    for i in range(n_steps+1):\n",
    "        \n",
    "        \n",
    "        pred = model.predict(start.reshape(1, W, 1))\n",
    "        if verbose == 2:\n",
    "            print('{}: {:.3f}'.format(cl, float(pr)))\n",
    "        scores[i] = pred[0][cl]\n",
    "        clas[i] = np.argmax(pred[0])\n",
    "        if verbose == 2 or (verbose == 1 and i == n_steps) or save_to:\n",
    "            plt.figure(figsize=(14, 4))\n",
    "            plt.subplot(121)\n",
    "            plt.title('{} {:.1f}%, Pred Class:{}={:.4f}, Given Class:{}'.format(ylabel, 100 * i / n_steps, cl, scores[i],given_clas))\n",
    "            plt.axis('off')\n",
    "#             plt.plot(start)\n",
    "             \n",
    "            ################################################################################\n",
    "            \n",
    "            max_length = 6000\n",
    "            ts = test.reshape(1, test.shape[0],1)\n",
    "            \n",
    "            x = np.linspace(0, ts.shape[1] - 1, max_length, endpoint=True)\n",
    "            f = interp1d(range(ts.shape[1]), ts[0, :, 0])\n",
    "            y = f(x)\n",
    "\n",
    "            f = interp1d(range(ts.shape[1]), sa)\n",
    "            cas = f(x).astype(int)\n",
    "            plt.scatter(x,y, c=cas, cmap='jet', marker='.', s=20, vmin=0, vmax=100, linewidths=0.0)\n",
    "            if cbar:\n",
    "                plt.colorbar()\n",
    "        \n",
    "            ##################################################################################\n",
    "\n",
    "            plt.subplot(122)\n",
    "            plt.plot(np.arange(i+1) / n_steps, scores[:i+1])\n",
    "            plt.xlim(-0.1, 1.1)\n",
    "            plt.ylim(0, 1.05)\n",
    "            plt.fill_between(np.arange(i+1) / n_steps, 0, scores[:i+1], alpha=0.4)\n",
    "            plt.title('{} AUC Score={:.4f}'.format(title, auc(scores)))\n",
    "            \n",
    "            plt.xlabel(ylabel)\n",
    "            plt.ylabel('Prob Score')\n",
    "\n",
    "            if save_to:\n",
    "        \n",
    "                plt.savefig(save_path + \"UCI-\"+dataset+'-MIRROR-insertion-input-'+str(inp_n)+'-win-'+str(w_s)+'-class-'+str(cl)+'.pdf')\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()\n",
    "        if i < n_steps:\n",
    "\n",
    "            coords = salient_order[step * i:step * (i + 1)]\n",
    "            start[coords] = finish[coords]\n",
    "        \n",
    "    \n",
    "    return scores\n",
    "#     print(scores)\n",
    "#     print('AUC Score=',auc(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets from model folders\n",
    "datasets = pd.read_csv('/rhome/s7thakur/time-series-models/DataSummary.csv')['Name']\n",
    "inception = glob.glob(os.path.join('/rhome/s7thakur/time-series-models/inception/*'))\n",
    "resnet = glob.glob(os.path.join('/rhome/s7thakur/time-series-models/resnet/*'))\n",
    "cnn = glob.glob(os.path.join('/rhome/s7thakur/time-series-models/cnn/*'))\n",
    "autoencoder = glob.glob(os.path.join('/rhome/s7thakur/time-series-models/autoencoder/*'))\n",
    "# extrace the dataset names\n",
    "\n",
    "inception = [f.split('/')[5] for f in inception]\n",
    "autoencoder = [f.split('/')[5] for f in autoencoder]\n",
    "resnet = [f.split('/')[5] for f in resnet]\n",
    "cnn = [f.split('/')[5] for f in cnn]\n",
    "# create dataframe of the lists\n",
    "inception = pd.DataFrame(inception)\n",
    "autoencoder = pd.DataFrame(autoencoder)\n",
    "resnet = pd.DataFrame(resnet)\n",
    "cnn = pd.DataFrame(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets from model folders\n",
    "datasets = pd.read_csv('/rhome/s7thakur/ResNet1D/DataSummary.csv')['Name']\n",
    "resnet = glob.glob(os.path.join('/rhome/s7thakur/'))\n",
    "autoencoder = glob.glob(os.path.join('/rhome/s7thakur/ResNet1D/autoencoder/*'))\n",
    "# extrace the dataset names\n",
    "autoencoder = [f.split('/')[5] for f in autoencoder]\n",
    "# resnet = [f.split('/')[5] for f in resnet]\n",
    "# create dataframe of the lists\n",
    "autoencoder = pd.DataFrame(autoencoder)\n",
    "# resnet = pd.DataFrame(resnet)\n",
    "dir = '/rhome/s7thakur/ResNet1D'\n",
    "\n",
    "sal_dir = '/rhome/s7thakur/MIRROR/logs/vae/saliency/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "models = ['resnet', 'cnn', 'inception']\n",
    "# models = ['resnet']\n",
    "datasets = ['PhalangesOutlinesCorrect', 'CinCECGTorso','ItalyPowerDemand', 'Trace', 'GunPoint', 'GunPointAgeSpan', \n",
    "                            'Strawberry', 'ECGFiveDays', 'TwoLeadECG', 'Chinatown', 'DistalPhalanxOutlineCorrect']\n",
    "\n",
    "# datasets = ['Trace']\n",
    "for model in models:\n",
    "    \n",
    "    for dataset in datasets:\n",
    "#         dataset = dataset[0]if model == 'resnet':\n",
    "        \n",
    "        x_train, y_train, x_test, y_test = read_dataset(archive_dir, dataset)\n",
    "        x_train, x_test = reshape(x_train, x_test)\n",
    "        y_train, y_test = label_encoding(y_train, y_test)\n",
    "\n",
    "        if x_train.shape[0] < x_test.shape[0]:\n",
    "            test = x_train[0:50]\n",
    "            labels = y_train[0:50]\n",
    "\n",
    "        else:\n",
    "            test = x_test[0:50]\n",
    "            labels = y_test[0:50]\n",
    "\n",
    "\n",
    "        X = test\n",
    "        Y = np.argmax(labels,axis=1)\n",
    "        classes = np.unique(Y)\n",
    "\n",
    "        window_len = x_test.shape[1] - 10\n",
    "        if window_len%2 == 0:\n",
    "            window_len = window_len+1\n",
    "        \n",
    "        \n",
    "        sal_path = os.path.join(sal_dir,dataset+'-'+model+'.npy')\n",
    "        sal = np.load(sal_path)\n",
    "#         print(sal.shape)\n",
    "#         sal = np.abs(sal)\n",
    "        from scipy.signal import savgol_filter\n",
    "\n",
    "        X = test\n",
    "        # Y = np.argmax(labels,axis=1)\n",
    "        classes = np.unique(Y)\n",
    "\n",
    "        print(X.shape, Y.shape, sal.shape, window_len)\n",
    "\n",
    "        for c in classes:\n",
    "\n",
    "            plt.figure(figsize=(10,5))        \n",
    "            c_x = X[np.where(Y == c)]\n",
    "            c_sal = sal[np.where(Y==c)]\n",
    "            print('Dataset:',dataset)\n",
    "            for sa, test_input in zip(c_sal, c_x):\n",
    "                plt.plot(sa)\n",
    "            plt.show()\n",
    "    \n",
    "            plt.figure(figsize=(10,5))        \n",
    "#             c_sal = np.abs(c_sal)\n",
    "            for sa, test_input in zip(c_sal, c_x):\n",
    "\n",
    "                \n",
    "                sa = savgol_filter(sa[:,0], 21, 1) # window size 51, polynomial order 3\n",
    "                \n",
    "                max_length = 4000\n",
    "                minimum = np.mean(sa)\n",
    "\n",
    "                sa = sa - minimum\n",
    "                sa = sa / np.max(sa)\n",
    "                sa= sa * 100\n",
    "\n",
    "                ts = test_input.reshape(1, test.shape[1],1)\n",
    "                x = np.linspace(0, ts.shape[1] - 1, max_length, endpoint=True)\n",
    "                f = interp1d(range(ts.shape[1]), ts[0, :, 0])\n",
    "                y = f(x)\n",
    "\n",
    "                f = interp1d(range(ts.shape[1]), sa)\n",
    "                cas = f(x).astype(int)\n",
    "        #         plt.plot(cas)\n",
    "                plt.scatter(x=x, y=y, c=cas, cmap='jet', marker='.', s=10, vmin=0, vmax=100, linewidths=0.0)\n",
    "\n",
    "                plt.title('Model:{}, Dataset:{}, Class:{}'.format(model, dataset, c), fontsize=20)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# models = ['resnet', 'cnn', 'inception']\n",
    "models = ['resnet']\n",
    "datasets = ['PhalangesOutlinesCorrect', 'CinCECGTorso','ItalyPowerDemand', 'Trace', 'GunPoint', 'GunPointAgeSpan', \n",
    "                            'Strawberry', 'ECGFiveDays', 'TwoLeadECG', 'Chinatown', 'DistalPhalanxOutlineCorrect']\n",
    "\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    for dataset in datasets:\n",
    "#         dataset = dataset[0]if model == 'resnet':\n",
    "         \n",
    "        x_train, y_train, x_test, y_test = read_dataset(archive_dir, dataset)\n",
    "        x_train, x_test = reshape(x_train, x_test)\n",
    "        \n",
    "        if(x_train.shape[0] == 0): continue\n",
    "        if(x_test.shape[0] == 0): continue\n",
    "        \n",
    "#         y_train, y_test = label_encoding(y_train, y_test)\n",
    "\n",
    "        # transform to binary labels\n",
    "        enc = sklearn.preprocessing.OneHotEncoder()\n",
    "        enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
    "        y_train_binary = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
    "\n",
    "        if x_train.shape[0] < x_test.shape[0]:\n",
    "            \n",
    "            if x_train.shape[0] > 50:\n",
    "                test = x_train[0:50]\n",
    "                labels = y_train[0:50]\n",
    "            elif x_train.shape[0] >0:\n",
    "                test = x_train\n",
    "                labels = y_train\n",
    "            else: \n",
    "                print('Length zero')\n",
    "                continue\n",
    "        else:\n",
    "            if x_test.shape[0]>50:\n",
    "                test = x_test[0:50]\n",
    "                labels = y_test[0:50]\n",
    "            elif x_test.shape[0]>0:\n",
    "                test = x_test\n",
    "                labels = y_test\n",
    "            else:\n",
    "                print('Length zero')\n",
    "                continue\n",
    "            \n",
    "#         X = test.reshape(test.shape[0], test.shape[1], 1)\n",
    "        \n",
    "#         Y = np.argmax(labels,axis=1)\n",
    "        classes = np.unique(np.concatenate((y_train, y_test), axis=0))\n",
    "        print(classes)\n",
    "        model = keras.models.load_model(\n",
    "            '/rhome/s7thakur/ResNet1D/resnet' + '/' + dataset + '/best_model.hdf5')\n",
    "\n",
    "        print(labels.shape, test.shape)\n",
    "        # filters\n",
    "        w_k_c = model.layers[-1].get_weights()[0]  # weights for each filter k for each class c\n",
    "\n",
    "        # the same input\n",
    "        new_input_layer = model.inputs\n",
    "        # output is both the original as well as the before last layer\n",
    "        new_output_layer = [model.layers[-3].output, model.layers[-1].output]\n",
    "\n",
    "        new_feed_forward = keras.backend.function(new_input_layer, new_output_layer)\n",
    "\n",
    "\n",
    "        for c in classes:\n",
    "            plt.figure(figsize=(10,5))\n",
    "            count = 0\n",
    "            c_x_train = test[np.where(labels == c)]\n",
    "            if c_x_train.shape[0] == 0: continue\n",
    "#             print(c_x_train.shape)\n",
    "            for ts in c_x_train:\n",
    "                ts = ts.reshape(1, -1, 1)\n",
    "                [conv_out, predicted] = new_feed_forward([ts])\n",
    "        #         print(conv_out.shape)\n",
    "                pred_label = np.argmax(predicted)\n",
    "                orig_label = np.argmax(enc.transform([[c]]))\n",
    "                if pred_label == orig_label:\n",
    "                    cas = np.zeros(dtype=np.float, shape=(conv_out.shape[1]))\n",
    "                    for k, w in enumerate(w_k_c[:, orig_label]):\n",
    "                        cas += w * conv_out[0, :, k]\n",
    "        #                 print(conv_out[0, :, k].shape, w)\n",
    "                    minimum = np.min(cas)\n",
    "\n",
    "                    cas = cas - minimum\n",
    "\n",
    "                    cas = cas / max(cas)\n",
    "                    cas = cas * 100\n",
    "                    \n",
    "                    x = np.linspace(0, ts.shape[1] - 1, max_length, endpoint=True)\n",
    "                    # linear interpolation to smooth\n",
    "                    f = interp1d(range(ts.shape[1]), ts[0, :, 0])\n",
    "                    y = f(x)\n",
    "                    # if (y < -2.2).any():\n",
    "                    #     continue\n",
    "                    f = interp1d(range(ts.shape[1]), cas)\n",
    "                    cas = f(x).astype(int)\n",
    "                    plt.scatter(x=x, y=y, c=cas, cmap='jet', marker='.', s=2, vmin=0, vmax=100, linewidths=0.0)\n",
    "                    if dataset_name == 'Gun_Point':\n",
    "                        if c == 1:\n",
    "                            plt.yticks([-1.0, 0.0, 1.0, 2.0])\n",
    "                        else:\n",
    "                            plt.yticks([-2, -1.0, 0.0, 1.0, 2.0])\n",
    "                    count += 1\n",
    "                \n",
    "            cbar = plt.colorbar()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAM generated saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = '/rhome/s7thakur/'\n",
    "# root_dir = '/home/s7thakur/ecresearch-shared/compute1'\n",
    "\n",
    "# classifier = 'ResNet1D/resnet'\n",
    "# archive_name = 'UCRArchive/UCRArchive_2018'\n",
    "# dataset_name = 'TwoLeadECG'\n",
    "\n",
    "\n",
    "# max_length = 2000\n",
    "# x_train, y_train, x_test, y_test = read_dataset(os.path.join(root_dir, archive_name), dataset_name)\n",
    "\n",
    "# plt.plot()\n",
    "# for x in x_train[0:10]:\n",
    "#     plt.plot(x)\n",
    "# plt.show()\n",
    "# # transform to binary labels\n",
    "# enc = sklearn.preprocessing.OneHotEncoder()\n",
    "# enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
    "# y_train_binary = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
    "\n",
    "# x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "\n",
    "# model = keras.models.load_model(\n",
    "#     root_dir + '/' + classifier + '/' + dataset_name + '/best_model.hdf5')\n",
    "\n",
    "# filters\n",
    "w_k_c = model.layers[-1].get_weights()[0]  # weights for each filter k for each class c\n",
    "print(y_train.shape, x_train.shape)\n",
    "# the same input\n",
    "new_input_layer = model.inputs\n",
    "# output is both the original as well as the before last layer\n",
    "new_output_layer = [model.layers[-3].output, model.layers[-1].output]\n",
    "\n",
    "new_feed_forward = keras.backend.function(new_input_layer, new_output_layer)\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "\n",
    "for c in classes:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    count = 0\n",
    "    c_x_train = x_train[np.where(y_train == c)]\n",
    "    for ts in c_x_train:\n",
    "        ts = ts.reshape(1, -1, 1)\n",
    "        [conv_out, predicted] = new_feed_forward([ts])\n",
    "#         print(conv_out.shape)\n",
    "        pred_label = np.argmax(predicted)\n",
    "        orig_label = np.argmax(enc.transform([[c]]))\n",
    "        if pred_label == orig_label:\n",
    "            cas = np.zeros(dtype=np.float, shape=(conv_out.shape[1]))\n",
    "            for k, w in enumerate(w_k_c[:, orig_label]):\n",
    "                cas += w * conv_out[0, :, k]\n",
    "#                 print(conv_out[0, :, k].shape, w)\n",
    "\n",
    "            minimum = np.min(cas)\n",
    "\n",
    "            cas = cas - minimum\n",
    "\n",
    "            cas = cas / max(cas)\n",
    "            cas = cas * 100\n",
    "#             print(ts[0,:,0].shape, cas.shape)\n",
    "            \n",
    "#             compute_metric_score(ts[0,:,:], np.abs(cas),\"Trace\",0,5,5)\n",
    "            x = np.linspace(0, ts.shape[1] - 1, max_length, endpoint=True)\n",
    "            # linear interpolation to smooth\n",
    "            f = interp1d(range(ts.shape[1]), ts[0, :, 0])\n",
    "            y = f(x)\n",
    "            # if (y < -2.2).any():\n",
    "            #     continue\n",
    "            f = interp1d(range(ts.shape[1]), cas)\n",
    "            cas = f(x).astype(int)\n",
    "            plt.scatter(x=x, y=y, c=cas, cmap='jet', marker='.', s=2, vmin=0, vmax=100, linewidths=0.0)\n",
    "            if dataset_name == 'Gun_Point':\n",
    "                if c == 1:\n",
    "                    plt.yticks([-1.0, 0.0, 1.0, 2.0])\n",
    "                else:\n",
    "                    plt.yticks([-2, -1.0, 0.0, 1.0, 2.0])\n",
    "            count += 1\n",
    "\n",
    "    cbar = plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = '/rhome/s7thakur/'\n",
    "root_dir = '/home/s7thakur/ecresearch-shared/compute1'\n",
    "\n",
    "classifier = 'ResNet1D/resnet'\n",
    "archive_name = 'UCRArchive/UCRArchive_2018'\n",
    "dataset_name = 'TwoLeadECG'\n",
    "\n",
    "\n",
    "# max_length = 2000\n",
    "# x_train, y_train, x_test, y_test = read_dataset(os.path.join(root_dir, archive_name), dataset_name)\n",
    "\n",
    "# plt.plot()\n",
    "# for x in x_train[0:10]:\n",
    "#     plt.plot(x)\n",
    "# plt.show()\n",
    "# # transform to binary labels\n",
    "# enc = sklearn.preprocessing.OneHotEncoder()\n",
    "# enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
    "# y_train_binary = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
    "\n",
    "# x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "\n",
    "models = keras.models.load_model(\n",
    "    root_dir + '/' + classifier + '/' + dataset_name + '/best_model.hdf5')\n",
    "\n",
    "# filters\n",
    "print(models.layers[-1])\n",
    "print(models.layers[-3])\n",
    "# w_k_c = model.layers[-1].get_weights()[0]  # weights for each filter k for each class c\n",
    "# print(y_train.shape, x_train.shape)\n",
    "# # the same input\n",
    "# new_input_layer = model.inputs\n",
    "# # output is both the original as well as the before last layer\n",
    "# new_output_layer = [model.layers[-3].output, model.layers[-1].output]\n",
    "\n",
    "# new_feed_forward = keras.backend.function(new_input_layer, new_output_layer)\n",
    "\n",
    "# classes = np.unique(y_train)\n",
    "\n",
    "# for c in classes:\n",
    "#     plt.figure(figsize=(10,5))\n",
    "#     count = 0\n",
    "#     c_x_train = x_train[np.where(y_train == c)]\n",
    "#     for ts in c_x_train:\n",
    "#         ts = ts.reshape(1, -1, 1)\n",
    "#         [conv_out, predicted] = new_feed_forward([ts])\n",
    "# #         print(conv_out.shape)\n",
    "#         pred_label = np.argmax(predicted)\n",
    "#         orig_label = np.argmax(enc.transform([[c]]))\n",
    "#         if pred_label == orig_label:\n",
    "#             cas = np.zeros(dtype=np.float, shape=(conv_out.shape[1]))\n",
    "#             for k, w in enumerate(w_k_c[:, orig_label]):\n",
    "#                 cas += w * conv_out[0, :, k]\n",
    "# #                 print(conv_out[0, :, k].shape, w)\n",
    "\n",
    "#             minimum = np.min(cas)\n",
    "\n",
    "#             cas = cas - minimum\n",
    "\n",
    "#             cas = cas / max(cas)\n",
    "#             cas = cas * 100\n",
    "# #             print(ts[0,:,0].shape, cas.shape)\n",
    "            \n",
    "# #             compute_metric_score(ts[0,:,:], np.abs(cas),\"Trace\",0,5,5)\n",
    "#             x = np.linspace(0, ts.shape[1] - 1, max_length, endpoint=True)\n",
    "#             # linear interpolation to smooth\n",
    "#             f = interp1d(range(ts.shape[1]), ts[0, :, 0])\n",
    "#             y = f(x)\n",
    "#             # if (y < -2.2).any():\n",
    "#             #     continue\n",
    "#             f = interp1d(range(ts.shape[1]), cas)\n",
    "#             cas = f(x).astype(int)\n",
    "#             plt.scatter(x=x, y=y, c=cas, cmap='jet', marker='.', s=2, vmin=0, vmax=100, linewidths=0.0)\n",
    "#             if dataset_name == 'Gun_Point':\n",
    "#                 if c == 1:\n",
    "#                     plt.yticks([-1.0, 0.0, 1.0, 2.0])\n",
    "#                 else:\n",
    "#                     plt.yticks([-2, -1.0, 0.0, 1.0, 2.0])\n",
    "#             count += 1\n",
    "\n",
    "#     cbar = plt.colorbar()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['resnet', 'cnn', 'inception']\n",
    "\n",
    "datasets = dataset_names = ['PhalangesOutlinesCorrect', 'CinCECGTorso','ItalyPowerDemand', 'Trace', 'GunPoint', 'GunPointAgeSpan', \n",
    "                            'Strawberry', 'ECGFiveDays', 'TwoLeadECG', 'Chinatown', 'DistalPhalanxOutlineCorrect']\n",
    "\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    for dataset in autoencoder.values:\n",
    "#         dataset = dataset[0]if model == 'resnet':\n",
    "        if dataset not in resnet.values:\n",
    "            continue\n",
    "        if model == 'cnn':\n",
    "            if dataset not in cnn.values:\n",
    "                continue\n",
    "        if model == 'inception':\n",
    "            if dataset not in inception.values:\n",
    "                continue\n",
    "        print(dataset)\n",
    "        \n",
    "        x_train, y_train, x_test, y_test = read_dataset(archive_dir, dataset)\n",
    "        x_train, x_test = reshape(x_train, x_test)\n",
    "        y_train, y_test = label_encoding(y_train, y_test)\n",
    "\n",
    "        if x_train.shape[0] < x_test.shape[0]:\n",
    "            test = x_train[0:50]\n",
    "            labels = y_train[0:50]\n",
    "\n",
    "        else:\n",
    "            test = x_test[0:50]\n",
    "            labels = y_test[0:50]\n",
    "\n",
    "\n",
    "        X = test\n",
    "        Y = np.argmax(labels,axis=1)\n",
    "        classes = np.unique(Y)\n",
    "\n",
    "\n",
    "        window_len = x_test.shape[1] - 10\n",
    "        if window_len%2 == 0:\n",
    "            window_len = window_len+1\n",
    "        \n",
    "        \n",
    "        sal_path = os.path.join(dir, dataset+'-'+model+'.npy')\n",
    "        sal = np.load(sal_path)\n",
    "#         print(sal.shape)\n",
    "#         sal = np.abs(sal)\n",
    "        from scipy.signal import savgol_filter\n",
    "\n",
    "        X = test\n",
    "        # Y = np.argmax(labels,axis=1)\n",
    "        classes = np.unique(Y)\n",
    "\n",
    "        print(X.shape, Y.shape, sal.shape, window_len)\n",
    "\n",
    "        for c in classes:\n",
    "\n",
    "            plt.figure(figsize=(10,5))        \n",
    "            c_x = X[np.where(Y == c)]\n",
    "            c_sal = sal[np.where(Y==c)]\n",
    "            print('Dataset:',dataset)\n",
    "            for sa, test_input in zip(c_sal, c_x):\n",
    "                plt.plot(sa)\n",
    "            plt.show()\n",
    "    \n",
    "            plt.figure(figsize=(10,5))        \n",
    "            c_sal = np.abs(c_sal)\n",
    "            for sa, test_input in zip(c_sal, c_x):\n",
    "\n",
    "                sa = savgol_filter(sa[:,0], 71, 1) # window size 51, polynomial order 3\n",
    "                max_length = 4000\n",
    "                minimum = np.mean(sa)\n",
    "\n",
    "                sa = sa - minimum\n",
    "                sa = sa / np.max(sa)\n",
    "                sa= sa * 100\n",
    "\n",
    "                ts = test_input.reshape(1, test.shape[1],1)\n",
    "                x = np.linspace(0, ts.shape[1] - 1, max_length, endpoint=True)\n",
    "                f = interp1d(range(ts.shape[1]), ts[0, :, 0])\n",
    "                y = f(x)\n",
    "\n",
    "                f = interp1d(range(ts.shape[1]), sa)\n",
    "                cas = f(x).astype(int)\n",
    "        #         plt.plot(cas)\n",
    "                plt.scatter(x=x, y=y, c=cas, cmap='jet', marker='.', s=20, vmin=0, vmax=100, linewidths=0.0)\n",
    "\n",
    "                plt.title('class:{}'.format(c), fontsize=20)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### root_dir = '/rhome/s7thakur'\n",
    "classifier = 'ResNet1D'\n",
    "archive_name = 'UCRArchive/UCRArchive_2018'\n",
    "dataset_name = 'Trace'\n",
    "\n",
    "\n",
    "max_length = 2000\n",
    "x_train, y_train, x_test, y_test = read_dataset(os.path.join(root_dir, archive_name), dataset_name)\n",
    "\n",
    "plt.plot()\n",
    "for x in x_train[0:10]:\n",
    "    plt.plot(x)\n",
    "plt.show()\n",
    "# transform to binary labels\n",
    "enc = sklearn.preprocessing.OneHotEncoder()\n",
    "enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
    "y_train_binary = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "\n",
    "model = keras.models.load_model(\n",
    "    root_dir + '/' + classifier + '/' + dataset_name + '/best_model.hdf5')\n",
    "\n",
    "# filters\n",
    "w_k_c = model.layers[-1].get_weights()[0]  # weights for each filter k for each class c\n",
    "\n",
    "# the same input\n",
    "new_input_layer = model.inputs\n",
    "# output is both the original as well as the before last layer\n",
    "new_output_layer = [model.layers[-3].output, model.layers[-1].output]\n",
    "\n",
    "new_feed_forward = keras.backend.function(new_input_layer, new_output_layer)\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "\n",
    "for c in classes:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    count = 0\n",
    "    c_x_train = x_train[np.where(y_train == c)]\n",
    "    for ts in c_x_train:\n",
    "        ts = ts.reshape(1, -1, 1)\n",
    "        [conv_out, predicted] = new_feed_forward([ts])\n",
    "#         print(conv_out.shape)\n",
    "        pred_label = np.argmax(predicted)\n",
    "        orig_label = np.argmax(enc.transform([[c]]))\n",
    "        if pred_label == orig_label:\n",
    "            cas = np.zeros(dtype=np.float, shape=(conv_out.shape[1]))\n",
    "            for k, w in enumerate(w_k_c[:, orig_label]):\n",
    "                cas += w * conv_out[0, :, k]\n",
    "#                 print(conv_out[0, :, k].shape, w)\n",
    "\n",
    "            minimum = np.min(cas)\n",
    "\n",
    "            cas = cas - minimum\n",
    "\n",
    "            cas = cas / max(cas)\n",
    "            cas = cas * 100\n",
    "\n",
    "            x = np.linspace(0, ts.shape[1] - 1, max_length, endpoint=True)\n",
    "            # linear interpolation to smooth\n",
    "            f = interp1d(range(ts.shape[1]), ts[0, :, 0])\n",
    "            y = f(x)\n",
    "            # if (y < -2.2).any():\n",
    "            #     continue\n",
    "            f = interp1d(range(ts.shape[1]), cas)\n",
    "            cas = f(x).astype(int)\n",
    "            plt.scatter(x=x, y=y, c=cas, cmap='jet', marker='.', s=2, vmin=0, vmax=100, linewidths=0.0)\n",
    "            if dataset_name == 'Gun_Point':\n",
    "                if c == 1:\n",
    "                    plt.yticks([-1.0, 0.0, 1.0, 2.0])\n",
    "                else:\n",
    "                    plt.yticks([-2, -1.0, 0.0, 1.0, 2.0])\n",
    "            count += 1\n",
    "\n",
    "    cbar = plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception = [f.split('/')[5] for f in inception]\n",
    "autoencoder = [f.split('/')[5] for f in autoencoder]\n",
    "resnet = [f.split('/')[5] for f in resnet]\n",
    "cnn = [f.split('/')[5] for f in cnn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception = pd.DataFrame(inception)\n",
    "autoencoder = pd.DataFrame(autoencoder)\n",
    "resnet = pd.DataFrame(resnet)\n",
    "cnn = pd.DataFrame(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "my_str = \"hey123: th~!ere {} for !! example 20\"\n",
    "keywords = re.sub('[^a-zA-Z0-9 \\n\\.]', '', my_str)\n",
    "keywords = re.sub('\\s+',' ',keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = list(keywords.split(' '))\n",
    "keywords = '-'.join(keywords)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snakes",
   "language": "python",
   "name": "snakes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
